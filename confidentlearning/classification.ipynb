{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RankPruning algorithm class for multiclass learning with noisy labels. \n",
    "#### The RankPruning class wraps around an instantion of a classifier. Your classifier must adhere to the sklearn template, meaning it must define three functions:\n",
    "* clf.fit(X, y, sample_weight = None)\n",
    "* clf.predict_proba(X)\n",
    "* clf.predict(X)\n",
    "\n",
    "where 'X' (of length n) contains your data, 'y' (of length n) contains your targets formatted as 0, 1, 2, ..., K-1, and sample_weight (of length n) that reweights examples in the loss function while training.\n",
    "\n",
    "## Example\n",
    "\n",
    "```python\n",
    "from confidentlearning.classification import RankPruning\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "rp = RankPruning(clf = logreg())\n",
    "rp.fit(X_train, y_train_noisy)\n",
    "y_pred = rp.predict(X_test) # Predictions approximate rp.fit(X_train, y_train_no_noise)\n",
    "```\n",
    "## Notes\n",
    "\n",
    "* s - used to denote the noisy labels in the code\n",
    "* Class labels (K classes) must be formatted as natural numbers: 0, 1, 2, ..., K-1\n",
    "* Do not skip a natural number, i.e. 0, 1, 3, 4, .. is ***NOT*** okay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "from confidentlearning.util import assert_inputs_are_valid, value_counts, remove_noise_from_class\n",
    "from confidentlearning.latent_estimation import \\\n",
    "    estimate_py_noise_matrices_and_cv_pred_proba, \\\n",
    "    estimate_py_and_noise_matrices_from_probabilities, \\\n",
    "    estimate_cv_predicted_probabilities\n",
    "from confidentlearning.latent_algebra import compute_py_inv_noise_matrix, compute_noise_matrix_from_inverse\n",
    "from confidentlearning.pruning import get_noise_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RankPruning(object):\n",
    "    '''Rank Pruning is a state-of-the-art algorithm (2017) for \n",
    "      multiclass classification with (potentially extreme) mislabeling \n",
    "      across any or all pairs of class labels. It works with ANY classifier,\n",
    "      including deep neural networks. See clf parameter.\n",
    "    This subfield of machine learning is referred to as Confident Learning.\n",
    "    Rank Pruning also achieves state-of-the-art performance for binary\n",
    "      classification with noisy labels and positive-unlabeled\n",
    "      learning (PU learning) where a subset of positive examples is given and\n",
    "      all other examples are unlabeled and assumed to be negative examples.\n",
    "    Rank Pruning works by \"learning from confident examples.\" Confident examples are\n",
    "      identified as examples with high predicted probability for their training label.\n",
    "    Given any classifier having the predict_proba() method, an input feature matrix, X, \n",
    "      and a discrete vector of labels, s, which may contain mislabeling, Rank Pruning \n",
    "      estimates the classifications that would be obtained if the hidden, true labels, y,\n",
    "      had instead been provided to the classifier during training.\n",
    "    \"s\" denotes the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "    clf : sklearn.classifier or equivalent class\n",
    "      The clf object must have the following three functions defined:\n",
    "        1. clf.predict_proba(X) # Predicted probabilities\n",
    "        2. clf.predict(X) # Predict labels\n",
    "        3. clf.fit(X,y) # Train classifier\n",
    "      Stores the classifier used in Rank Pruning.\n",
    "      Default classifier used is logistic regression.\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.'''  \n",
    "  \n",
    "  \n",
    "    def __init__(self, clf = None, seed = None):\n",
    "        self.clf = logreg() if clf is None else clf\n",
    "        self.seed = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed = seed)\n",
    "  \n",
    "  \n",
    "    def fit(\n",
    "        self, \n",
    "        X,\n",
    "        s,\n",
    "        cv_n_folds = 5,\n",
    "        pulearning = None,\n",
    "        psx = None,\n",
    "        thresholds = None,\n",
    "        noise_matrix = None,\n",
    "        inverse_noise_matrix = None,\n",
    "        prune_method = 'prune_by_noise_rate',\n",
    "        prune_count_method = 'inverse_nm_dot_s',\n",
    "        converge_latent_estimates = False,\n",
    "        \n",
    "    ):\n",
    "        '''This method implements the Rank Pruning mantra 'learning with confident examples.'\n",
    "        This function fits the classifer (self.clf) to (X, s) accounting for the noise in\n",
    "        both the positive and negative sets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "          Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "        s : np.array\n",
    "          A binary vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "          the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "        cv_n_folds : int\n",
    "          The number of cross-validation folds used to compute\n",
    "          out-of-sample probabilities for each example in X.\n",
    "\n",
    "        pulearning : int\n",
    "          Set to the integer of the class that is perfectly labeled, if such\n",
    "          a class exists. Otherwise, or if you are unsure, \n",
    "          leave pulearning = None (default).\n",
    "\n",
    "        psx : np.array (shape (N, K))\n",
    "          P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "          This is the probability distribution over all K classes, for each\n",
    "          example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "          have been computed using 3 (or higher) fold cross-validation.\n",
    "          If you are not sure, leave psx = None (default) and\n",
    "          it will be computed for you using cross-validation.\n",
    "\n",
    "        thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "          P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "          this threshold, it is counted as having hidden label y = k. This is \n",
    "          not used for pruning, only for estimating the noise rates using \n",
    "          confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "        noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "          A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "          the fraction of examples in every class, labeled as every other class.\n",
    "          Assumes columns of noise_matrix sum to 1. \n",
    "    \n",
    "        inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "          A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "          the estimated fraction observed examples in each class k_s, that are\n",
    "          mislabeled examples from every other class k_y. If None, the \n",
    "          inverse_noise_matrix will be computed from psx and s.\n",
    "          Assumes columns of inverse_noise_matrix sum to 1.\n",
    "\n",
    "        prune_method : str\n",
    "          'prune_by_class', 'prune_by_noise_rate', or 'both'. Method used for pruning.\n",
    "          \n",
    "        prune_count_method : str (default 'inverse_nm_dot_s')\n",
    "          Options are 'inverse_nm_dot_s' or 'calibrate_confident_joint'. Method used to estimate the counts of the\n",
    "          joint P(s, y) that will be used to determine which how many examples to prune\n",
    "          for every class that are flipped to every other class.\n",
    "\n",
    "        converge_latent_estimates : bool (Default: False)\n",
    "          If true, forces numerical consistency of estimates. Each is estimated\n",
    "          independently, but they are related mathematically with closed form \n",
    "          equivalences. This will iteratively enforce mathematically consistency.\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "          Returns (noise_mask, sample_weight)'''\n",
    "    \n",
    "        # Check inputs\n",
    "        assert_inputs_are_valid(X, s, psx)\n",
    "        if noise_matrix is not None and np.trace(noise_matrix) <= 1:\n",
    "            raise Exception(\"Trace(noise_matrix) must exceed 1.\")\n",
    "        if inverse_noise_matrix is not None and np.trace(inverse_noise_matrix) <= 1:\n",
    "            raise Exception(\"Trace(inverse_noise_matrix) must exceed 1.\")\n",
    "\n",
    "        # Number of classes\n",
    "        self.K = len(np.unique(s))\n",
    "\n",
    "        # 'ps' is p(s=k)\n",
    "        self.ps = value_counts(s) / float(len(s))\n",
    "\n",
    "        self.confident_joint = None\n",
    "        # If needed, compute noise rates (fraction of mislabeling) for all classes. \n",
    "        # Also, if needed, compute P(s=k|x), denoted psx.\n",
    "        \n",
    "        if noise_matrix is not None:\n",
    "            self.noise_matrix = noise_matrix\n",
    "            if inverse_noise_matrix is None:\n",
    "                self.py, self.inverse_noise_matrix = compute_py_inv_noise_matrix(self.ps, self.noise_matrix)\n",
    "        if inverse_noise_matrix is not None:\n",
    "            self.inverse_noise_matrix = inverse_noise_matrix\n",
    "            if noise_matrix is None:\n",
    "                self.noise_matrix = compute_noise_matrix_from_inverse(self.ps, self.inverse_noise_matrix)\n",
    "        if noise_matrix is None and inverse_noise_matrix is None:\n",
    "            if psx is None:\n",
    "                self.py, self.noise_matrix, self.inverse_noise_matrix, self.confident_joint, psx = \\\n",
    "                estimate_py_noise_matrices_and_cv_pred_proba(\n",
    "                    X = X, \n",
    "                    s = s, \n",
    "                    clf = self.clf,\n",
    "                    cv_n_folds = cv_n_folds,\n",
    "                    thresholds = thresholds, \n",
    "                    converge_latent_estimates = converge_latent_estimates,\n",
    "                    seed = self.seed,\n",
    "                )\n",
    "            else: # psx is provided by user (assumed holdout probabilities)\n",
    "                self.py, self.noise_matrix, self.inverse_noise_matrix, self.confident_joint = \\\n",
    "                estimate_py_and_noise_matrices_from_probabilities(\n",
    "                    s = s, \n",
    "                    psx = psx,\n",
    "                    thresholds = thresholds, \n",
    "                    converge_latent_estimates = converge_latent_estimates,\n",
    "                )\n",
    "\n",
    "        if psx is None: \n",
    "            psx = estimate_cv_predicted_probabilities(\n",
    "                X = X, \n",
    "                labels = s, \n",
    "                clf = self.clf,\n",
    "                cv_n_folds = cv_n_folds,\n",
    "                seed = self.seed,\n",
    "            ) \n",
    "\n",
    "        # Zero out noise matrix entries if pulearning = the integer specifying the class without noise.\n",
    "        if pulearning is not None:\n",
    "            self.noise_matrix = remove_noise_from_class(self.noise_matrix, class_without_noise=pulearning)\n",
    "            # TODO: self.inverse_noise_matrix = remove_noise_from_class(self.inverse_noise_matrix, class_without_noise=pulearning)\n",
    "\n",
    "        # This is the actual work of this function.\n",
    "\n",
    "        # Get the indices of the examples we wish to prune\n",
    "        self.noise_mask = get_noise_indices(\n",
    "            s, \n",
    "            psx, \n",
    "            inverse_noise_matrix = self.inverse_noise_matrix, \n",
    "            confident_joint = self.confident_joint,\n",
    "            prune_method = prune_method, \n",
    "            prune_count_method = prune_count_method,\n",
    "            converge_latent_estimates = converge_latent_estimates,\n",
    "        ) \n",
    "\n",
    "        X_mask = ~self.noise_mask\n",
    "        X_pruned = X[X_mask]\n",
    "        s_pruned = s[X_mask]\n",
    "\n",
    "        # Re-weight examples in the loss function for the final fitting\n",
    "        # s.t. the \"apparent\" original number of examples in each class\n",
    "        # is preserved, even though the pruned sets may differ.\n",
    "        self.sample_weight = np.ones(np.shape(s_pruned))\n",
    "        for k in range(self.K): \n",
    "            self.sample_weight[s_pruned == k] = 1.0 / self.noise_matrix[k][k]\n",
    "\n",
    "        self.clf.fit(X_pruned, s_pruned, sample_weight=self.sample_weight)\n",
    "        return self.clf\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Returns a binary vector of predictions.'''\n",
    "\n",
    "        return self.clf.predict(X)\n",
    "  \n",
    "  \n",
    "    def predict_proba(self, X):\n",
    "        '''Returns a vector of probabilties P(y=k)\n",
    "        for each example in X.'''\n",
    "\n",
    "        return self.clf.predict_proba(X)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
