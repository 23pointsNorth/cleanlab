{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notes\n",
    "# -----\n",
    "\n",
    "# s - used to denote the noisy label \n",
    "# Typically,\\tilde(y) is used in the literature\n",
    "\n",
    "# Class labels (K classes) must be formatted as natural numbers: 0, 1, 2, ..., K-1\n",
    "# Do not skip a natural number, i.e. 0, 1, 3, 4, .. is ***NOT*** okay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Leave at least this many examples in each class after\n",
    "# pruning, regardless if noise estimates are larger.\n",
    "MIN_NUM_PER_CLASS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RankPruning(object):\n",
    "    '''Rank Pruning is a state-of-the-art algorithm (2017) for \n",
    "      multiclass classification with (potentially extreme) mislabeling \n",
    "      across any or all pairs of class labels. It works with ANY classifier,\n",
    "      including deep neural networks. See clf parameter.\n",
    "    This subfield of machine learning is referred to as Confident Learning.\n",
    "    Rank Pruning also achieves state-of-the-art performance for binary\n",
    "      classification with noisy labels and positive-unlabeled\n",
    "      learning (PU learning) where a subset of positive examples is given and\n",
    "      all other examples are unlabeled and assumed to be negative examples.\n",
    "    Rank Pruning works by \"learning from confident examples.\" Confident examples are\n",
    "      identified as examples with high predicted probability for their training label.\n",
    "    Given any classifier having the predict_proba() method, an input feature matrix, X, \n",
    "      and a discrete vector of labels, s, which may contain mislabeling, Rank Pruning \n",
    "      estimates the classifications that would be obtained if the hidden, true labels, y,\n",
    "      had instead been provided to the classifier during training.\n",
    "    \"s\" denotes the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "    clf : sklearn.classifier or equivalent class\n",
    "      The clf object must have the following three functions defined:\n",
    "        1. clf.predict_proba(X) # Predicted probabilities\n",
    "        2. clf.predict(X) # Predict labels\n",
    "        3. clf.fit(X,y) # Train classifier\n",
    "      Stores the classifier used in Rank Pruning.\n",
    "      Default classifier used is logistic regression.\n",
    "\n",
    "    noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "      A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "      the fraction of examples in every class, (mis)labeled as every other class.\n",
    "      Only provide this matrix if you know the fractions of mislabeling for all\n",
    "      pairs of classes already. Noise rates may be referred to as rho in literature.\n",
    "      Assumes columns of noise_matrix sum to 1.'''  \n",
    "  \n",
    "  \n",
    "    def __init__(self, clf = None):\n",
    "        self.clf = logreg() if clf is None else clf\n",
    "  \n",
    "  \n",
    "    def fit(\n",
    "        self, \n",
    "        X,\n",
    "        s,\n",
    "        cv_n_folds = 5,\n",
    "        pulearning = None,\n",
    "        psx = None,\n",
    "        thresholds = None,\n",
    "        noise_matrix = None,\n",
    "        inverse_noise_matrix = None,\n",
    "        method = 'prune_by_noise_rate',\n",
    "        converge_latent_estimates = False,\n",
    "        verbose = False,\n",
    "    ):\n",
    "        '''This method implements the Rank Pruning mantra 'learning with confident examples.'\n",
    "        This function fits the classifer (self.clf) to (X, s) accounting for the noise in\n",
    "        both the positive and negative sets.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "          Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "        s : np.array\n",
    "          A binary vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "          the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "        cv_n_folds : int\n",
    "          The number of cross-validation folds used to compute\n",
    "          out-of-sample probabilities for each example in X.\n",
    "\n",
    "        pulearning : int\n",
    "          Set to the integer of the class that is perfectly labeled, if such\n",
    "          a class exists. Otherwise, or if you are unsure, \n",
    "          leave pulearning = None (default).\n",
    "\n",
    "        psx : np.array (shape (N, K))\n",
    "          P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "          This is the probability distribution over all K classes, for each\n",
    "          example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "          have been computed using 3 (or higher) fold cross-validation.\n",
    "          If you are not sure, leave psx = None (default) and\n",
    "          it will be computed for you using cross-validation.\n",
    "\n",
    "        thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "          P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "          this threshold, it is counted as having hidden label y = k. This is \n",
    "          not used for pruning, only for estimating the noise rates using \n",
    "          confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "        noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "          A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "          the fraction of examples in every class, labeled as every other class.\n",
    "          Assumes columns of noise_matrix sum to 1. \n",
    "    \n",
    "        inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "          A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "          the estimated fraction observed examples in each class k_s, that are\n",
    "          mislabeled examples from every other class k_y. If None, the \n",
    "          inverse_noise_matrix will be computed from psx and s.\n",
    "          Assumes columns of inverse_noise_matrix sum to 1.\n",
    "\n",
    "        method : str\n",
    "          'prune_by_class', 'prune_by_noise_rate', or 'both'. Method used for pruning.\n",
    "\n",
    "        converge_latent_estimates : bool\n",
    "          If true, forces numerical consistency of estimates. Each is estimated\n",
    "          independently, but they are related mathematically with closed form \n",
    "          equivalences. This will iteratively enforce mathematically consistency. \n",
    "\n",
    "        verbose : bool\n",
    "          Set to true if you wish to print additional information while running.\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "          Returns (noise_mask, sample_weight)'''\n",
    "    \n",
    "        # Check inputs\n",
    "        assert_inputs_are_valid(X, s, psx)\n",
    "        if noise_matrix is not None and np.trace(noise_matrix) <= 1:\n",
    "            raise Exception(\"Trace(noise_matrix) must exceed 1.\")\n",
    "        if inverse_noise_matrix is not None and np.trace(inverse_noise_matrix) <= 1:\n",
    "            raise Exception(\"Trace(inverse_noise_matrix) must exceed 1.\")\n",
    "\n",
    "        # Number of classes\n",
    "        self.K = len(np.unique(s))\n",
    "\n",
    "        # 'ps' is p(s=k)\n",
    "        self.ps = value_counts(s) / float(len(s))\n",
    "\n",
    "        # If needed, compute noise rates (fraction of mislabeling) for all classes. \n",
    "        # Also, if needed, compute P(s=k|x), denoted psx.\n",
    "        \n",
    "        if noise_matrix is not None:\n",
    "            self.noise_matrix = noise_matrix\n",
    "            if inverse_noise_matrix is None:\n",
    "                self.py, self.inverse_noise_matrix = compute_py_inv_noise_matrix(self.ps, self.noise_matrix)\n",
    "        if inverse_noise_matrix is not None:\n",
    "            self.inverse_noise_matrix = inverse_noise_matrix\n",
    "            if noise_matrix is None:\n",
    "                self.noise_matrix = compute_noise_matrix_from_inverse(self.ps, self.inverse_noise_matrix)\n",
    "        if noise_matrix is None and inverse_noise_matrix is None:\n",
    "            if psx is None:\n",
    "                self.py, self.noise_matrix, self.inverse_noise_matrix, psx = \\\n",
    "                compute_py_noise_matrices_and_cv_pred_proba(\n",
    "                    X = X, \n",
    "                    s = s, \n",
    "                    clf = self.clf,\n",
    "                    cv_n_folds = cv_n_folds,\n",
    "                    thresholds = thresholds, \n",
    "                    converge_latent_estimates = converge_latent_estimates,\n",
    "                )\n",
    "            else: # psx is provided by user (assumed holdout probabilities)\n",
    "                self.py, self.noise_matrix, self.inverse_noise_matrix, _ = \\\n",
    "                estimate_py_and_noise_matrices_from_probabilities(\n",
    "                    s = s, \n",
    "                    psx = psx,\n",
    "                    thresholds = thresholds, \n",
    "                    converge_latent_estimates = converge_latent_estimates,\n",
    "                )\n",
    "\n",
    "        if psx is None: \n",
    "            psx = compute_cv_predicted_probabilities(\n",
    "                X = X, \n",
    "                labels = s, \n",
    "                clf = self.clf,\n",
    "                cv_n_folds = cv_n_folds,\n",
    "                verbose = verbose,\n",
    "            ) \n",
    "\n",
    "        # Zero out noise matrix entries if pulearning = the integer specifying the class without noise.\n",
    "        if pulearning is not None:\n",
    "            self.noise_matrix = remove_noise_from_class(self.noise_matrix, class_without_noise=pulearning)\n",
    "            # TODO: self.inverse_noise_matrix = remove_noise_from_class(self.inverse_noise_matrix, class_without_noise=pulearning)\n",
    "\n",
    "        # This is the actual work of this function.\n",
    "\n",
    "        # Get the indices of the examples we wish to prune\n",
    "        self.noise_mask = get_noise_indices(s, psx, self.inverse_noise_matrix, method=method)\n",
    "\n",
    "        X_mask = ~self.noise_mask\n",
    "        X_pruned = X[X_mask]\n",
    "        s_pruned = s[X_mask]\n",
    "\n",
    "        # Re-weight examples in the loss function for the final fitting\n",
    "        # s.t. the \"apparent\" original number of examples in each class\n",
    "        # is preserved, even though the pruned sets may differ.\n",
    "        self.sample_weight = np.ones(np.shape(s_pruned))\n",
    "        for k in range(self.K): \n",
    "            self.sample_weight[s_pruned == k] = 1.0 / self.noise_matrix[k][k]\n",
    "\n",
    "        self.clf.fit(X_pruned, s_pruned, sample_weight=self.sample_weight)\n",
    "\n",
    "        return self.clf\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''Returns a binary vector of predictions.'''\n",
    "\n",
    "        return self.clf.predict(X)\n",
    "  \n",
    "  \n",
    "    def predict_proba(self, X):\n",
    "        '''Returns a vector of probabilties P(y=k)\n",
    "        for each example in X.'''\n",
    "\n",
    "        return self.clf.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rank Pruning specific helper functions exposed to rankpruning module. \n",
    "\n",
    "def assert_inputs_are_valid(X, s, psx = None):\n",
    "    '''Checks that X, s, and psx\n",
    "    are correctly formatted'''\n",
    "\n",
    "    if psx is not None:\n",
    "        if not isinstance(psx, (np.ndarray, np.generic)):\n",
    "            raise TypeError(\"psx should be a numpy array.\")\n",
    "    if len(psx) != len(s):\n",
    "        raise ValueError(\"psx and s must have same length.\")\n",
    "    # Check for valid probabilities.\n",
    "    if (psx < 0).any() or (psx > 1).any():\n",
    "        raise ValueError(\"Values in psx must be between 0 and 1.\")\n",
    "\n",
    "    if not isinstance(s, (np.ndarray, np.generic)):\n",
    "        raise TypeError(\"s should be a numpy array.\")\n",
    "    if not isinstance(X, (np.ndarray, np.generic)):\n",
    "        raise TypeError(\"X should be a numpy array.\")\n",
    "    \n",
    "    \n",
    "def remove_noise_from_class(noise_matrix, class_without_noise):\n",
    "    '''A helper function in the setting of PU learning.\n",
    "    Sets all P(s=class_without_noise|y=any_other_class) = 0\n",
    "    in noise_matrix for pulearning setting, where we have \n",
    "    generalized the positive class in PU learning to be any\n",
    "    class of choosing, denoted by class_without_noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "        the fraction of examples in every class, labeled as every other class.\n",
    "        Assumes columns of noise_matrix sum to 1.\n",
    "\n",
    "    class_without_noise : int\n",
    "        Integer value of the class that has no noise. Traditionally,\n",
    "        this is 1 (positive) for PU learning.'''\n",
    "  \n",
    "    # Number of classes\n",
    "    K = len(noise_matrix)\n",
    "\n",
    "    cwn = class_without_noise\n",
    "    x = np.copy(noise_matrix)\n",
    "\n",
    "    # Set P( s = cwn | y != cwn) = 0 (no noise)\n",
    "    x[cwn, [i for i in range(K) if i!=cwn]] = 0.0\n",
    "\n",
    "    # Normalize columns by increasing diagnol terms\n",
    "    # Ensures noise_matrix is a valid probability matrix\n",
    "    for i in range(K):\n",
    "        x[i][i] = 1 - float(np.sum(x[:,i]) - x[i][i])\n",
    "\n",
    "    return x\n",
    "    \n",
    "\n",
    "def estimate_joint_counts_from_probabilities(\n",
    "    s, \n",
    "    psx, \n",
    "    thresholds = None, \n",
    "    force_ps = False,\n",
    "    return_list_of_converging_jc_matrices = False,\n",
    "):\n",
    "    '''Estimates P(s,y), the confident counts of the latent \n",
    "    joint distribution of true and noisy labels \n",
    "    using observed s and predicted probabilities psx.\n",
    "\n",
    "    Important! This function assumes that psx are out-of-sample \n",
    "    holdout probabilities. This can be done with cross validation. If\n",
    "    the probabilities are not computed out-of-sample, overfitting may occur.\n",
    "\n",
    "    This function estimates the joint of shape (K, K). This is the\n",
    "    confident counts of examples in every class, labeled as every other class.\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, the estimate is within 1 percent of the truth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "        A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "        the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    psx : np.array (shape (N, K))\n",
    "        P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "        This is the probability distribution over all K classes, for each\n",
    "        example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "        have been computed using 3 (or higher) fold cross-validation.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "        P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "        this threshold, it is counted as having hidden label y = k. This is \n",
    "        not used for pruning, only for estimating the noise rates using \n",
    "        confident counts. This value should be between 0 and 1. Default is None.\n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output joint_count matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the joint_count, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    return_list_of_converging_jc_matrices : bool (default = False)\n",
    "        When force_ps is true, it converges the joint count matrix that is returned.\n",
    "        Setting this to true will return the list of the converged matrices. The first\n",
    "        item in the list is the original and the last item is the final result.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "        joint_count matrix count(s, y) : np.array (shape (K, K))'''\n",
    "    \n",
    "    # Number of classes\n",
    "    K = len(np.unique(s))  \n",
    "    # 'ps' is p(s=k)\n",
    "    ps = value_counts(s) / float(len(s))\n",
    "    # joint counts\n",
    "    jcs = []\n",
    "    \n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "    \n",
    "    sgd_epochs = force_ps if type(force_ps) == int else 1   \n",
    "    for sgd_iteration in range(sgd_epochs): \n",
    "        # Estimate the probability thresholds for confident counting \n",
    "        if thresholds is None:\n",
    "            thresholds = [np.mean(psx[:,k][s == k]) for k in range(K)] # P(s^=k|s=k)\n",
    "\n",
    "        # Confident examples are those that we are confident have label y = k\n",
    "        # Estimate the (K, K) matrix of confident examples having s = k_s and y = k_y\n",
    "        joint_count = np.zeros((K,K))\n",
    "        for k_s in range(K): # k_s is the class value k of noisy label s\n",
    "            for k_y in range(K): # k_y is the (guessed) class value k of true label y\n",
    "                joint_count[k_s][k_y] = sum((psx[:,k_y] >= thresholds[k_y]) & (s == k_s))\n",
    "        jcs.append(joint_count)\n",
    "        \n",
    "        if force_ps:\n",
    "            joint_ps = joint_count.sum(axis=1) / float(np.sum(joint_count))\n",
    "            # Update thresholds (SGD) to converge p(s) of joint with actual p(s)    \n",
    "            eta = 0.5 # learning rate\n",
    "            thresholds += eta * (joint_ps - ps)\n",
    "        else: # Do not converge p(s) of joint with actual p(s)\n",
    "            break\n",
    "            \n",
    "    return jcs if return_list_of_converging_jc_matrices else joint_count\n",
    " \n",
    "    \n",
    "def estimate_latent(\n",
    "    joint_count, \n",
    "    s, \n",
    "    py_method = 'cnt', \n",
    "    converge_latent_estimates = False,\n",
    "):\n",
    "    '''Computes the latent prior p(y), the noise matrix P(s|y) and the\n",
    "    inverse noise matrix P(y|s) from the joint_count count(s, y). The\n",
    "    joint_count estimated by estimate_joint_counts_from_probabilities()\n",
    "    by counting confident examples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "        A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "        the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "        \n",
    "    joint_count : np.array (shape (K, K), type int)\n",
    "        A K,K integer matrix of count(s=k, y=k). Captures the joint disribution of\n",
    "        the noisy and true labels P_{s,y}. Each entry in the matrix contains\n",
    "        the number of examples confidently counted into both classes.\n",
    "        \n",
    "    py_method : str\n",
    "        How to compute the latent prior p(y=k). Default is \"cnt\" as it tends to\n",
    "        work best, but you may also set this hyperparameter to \"eqn\" or \"marginal\".\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent. '''\n",
    "    \n",
    "    # Number of classes\n",
    "    K = len(np.unique(s))  \n",
    "    # 'ps' is p(s=k)\n",
    "    ps = value_counts(s) / float(len(s))\n",
    "    \n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "    \n",
    "    # Number of training examples confidently counted from each noisy class\n",
    "    s_count = joint_count.sum(axis=1).astype(float)\n",
    "    \n",
    "    # Number of training examples confidently counted into each true class\n",
    "    y_count = joint_count.sum(axis=0).astype(float)\n",
    "    \n",
    "    # Confident Counts Estimator for p(s=k_s|y=k_y) ~ |s=k_s and y=k_y| / |y=k_y|\n",
    "    noise_matrix = joint_count / y_count\n",
    "\n",
    "    # Confident Counts Estimator for p(y=k_y|s=k_s) ~ |y=k_y and s=k_s| / |s=k_s|\n",
    "    inv_noise_matrix = joint_count.T / s_count\n",
    "    \n",
    "    if py_method == 'cnt': \n",
    "        py = (y_count / s_count) * ps\n",
    "        # Equivalently,\n",
    "        # py = inv_noise_matrix.diagonal() / noise_matrix.diagonal() * ps\n",
    "    elif py_method == 'eqn':\n",
    "        py = np.linalg.inv(noise_matrix).dot(ps)\n",
    "    elif py_method == 'marginal':\n",
    "        py = y_count / float(sum(y_count))\n",
    "    else:\n",
    "        raise ValueError('py_method parameter should be cnt, eqn, or marginal')\n",
    "    \n",
    "    # Clip py and noise rates into proper range [0,1)\n",
    "    # For py, no class should have probability 0 so we use 1e-5\n",
    "    py = clip_values(py, low=1e-5, high=1.0, new_sum = 1.0)\n",
    "    noise_matrix = clip_noise_rates(noise_matrix) \n",
    "    inv_noise_matrix = clip_noise_rates(inv_noise_matrix)\n",
    "\n",
    "    if converge_latent_estimates:\n",
    "        py, noise_matrix, inv_noise_matrix = converge_estimates(ps, py, noise_matrix, inv_noise_matrix)\n",
    "        # Again clip py and noise rates into proper range [0,1)\n",
    "        py = clip_values(py, low=1e-5, high=1.0, new_sum = 1.0) \n",
    "        noise_matrix = clip_noise_rates(noise_matrix) \n",
    "        inv_noise_matrix = clip_noise_rates(inv_noise_matrix)\n",
    "\n",
    "    return py, noise_matrix, inv_noise_matrix                  \n",
    "    \n",
    "    \n",
    "def estimate_py_and_noise_matrices_from_probabilities(\n",
    "    s, \n",
    "    psx, \n",
    "    thresholds = None,\n",
    "    converge_latent_estimates = True,\n",
    "    force_ps = False,\n",
    "    py_method = 'cnt', \n",
    "):\n",
    "    '''Computes the confident counts\n",
    "    estimate of latent variables py and the noise rates \n",
    "    using observed s and predicted probabilities psx.\n",
    "\n",
    "    Important! This function assumes that psx are out-of-sample \n",
    "    holdout probabilities. This can be done with cross validation. If\n",
    "    the probabilities are not computed out-of-sample, overfitting may occur.\n",
    "\n",
    "    This function estimates the noise_matrix of shape (K, K). This is the\n",
    "    fraction of examples in every class, labeled as every other class. The\n",
    "    noise_matrix is a conditional probability matrix for P(s=k_s|y=k_y).\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    psx : np.array (shape (N, K))\n",
    "      P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "      This is the probability distribution over all K classes, for each\n",
    "      example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "      have been computed using 3 (or higher) fold cross-validation.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent. \n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output joint_count matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the joint_count, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    py_method : str\n",
    "        How to compute the latent prior p(y=k). Default is \"cnt\" as it tends to\n",
    "        work best, but you may also set this hyperparameter to \"eqn\" or \"marginal\".\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "        py, noise_matrix, inverse_noise_matrix'''\n",
    "  \n",
    "    joint_count = estimate_joint_counts_from_probabilities(s, psx, thresholds, force_ps)\n",
    "    py, noise_matrix, inv_noise_matrix = estimate_latent(        \n",
    "        joint_count=joint_count, \n",
    "        s=s, \n",
    "        py_method=py_method, \n",
    "        converge_latent_estimates=converge_latent_estimates,\n",
    "    )\n",
    "    \n",
    "    return py, noise_matrix, inv_noise_matrix, joint_count\n",
    "\n",
    "\n",
    "def estimate_joint_counts_and_cv_pred_proba(\n",
    "    X, \n",
    "    s, \n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    thresholds = None,\n",
    "    force_ps = False,\n",
    "    return_list_of_converging_jc_matrices = False,\n",
    "    seed = None,\n",
    "):\n",
    "    '''Estimates P(s,y), the confident counts of the latent \n",
    "    joint distribution of true and noisy labels \n",
    "    using observed s and predicted probabilities psx. \n",
    "\n",
    "    The output of this function is a numpy array of shape (K, K). \n",
    "\n",
    "    Under certain conditions, estimates are exact, and in many\n",
    "    conditions, estimates are within one percent of actual.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output joint_count matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the joint_count, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    return_list_of_converging_jc_matrices : bool (default = False)\n",
    "        When force_ps is true, it converges the joint count matrix that is returned.\n",
    "        Setting this to true will return the list of the converged matrices. The first\n",
    "        item in the list is the original and the last item is the final result.\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "      Returns a tuple of two numpy array matrices in the form:\n",
    "      (joint counts matrix, predicted probability matrix)'''\n",
    "  \n",
    "    # Number of classes\n",
    "    K = len(np.unique(s))  \n",
    "    # 'ps' is p(s=k)\n",
    "    ps = value_counts(s) / float(len(s))\n",
    "    \n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "\n",
    "    # Create cross-validation object for out-of-sample predicted probabilities.\n",
    "    # CV folds preserve the fraction of noisy positive and\n",
    "    # noisy negative examples in each class.\n",
    "    kf = StratifiedKFold(n_splits = cv_n_folds, shuffle = True, random_state = seed)\n",
    "\n",
    "    # Intialize result storage and final psx array\n",
    "#     py_per_cv_fold = []\n",
    "#     noise_matrix_per_cv_fold = []\n",
    "#     inv_noise_matrix_per_cv_fold = []\n",
    "    joint_count_per_cv_fold = []\n",
    "    psx = np.zeros((len(s), K))\n",
    "\n",
    "    # Split X and s into \"cv_n_folds\" stratified folds.\n",
    "    for k, (cv_train_idx, cv_holdout_idx) in enumerate(kf.split(X, s)):\n",
    "\n",
    "        # Select the training and holdout cross-validated sets.\n",
    "        X_train_cv, X_holdout_cv = X[cv_train_idx], X[cv_holdout_idx]\n",
    "        s_train_cv, s_holdout_cv = s[cv_train_idx], s[cv_holdout_idx]\n",
    "\n",
    "        # Fit the clf classifier to the training set and \n",
    "        # predict on the holdout set and update psx. \n",
    "        clf.fit(X_train_cv, s_train_cv)\n",
    "        psx_cv = clf.predict_proba(X_holdout_cv) # P(s = k|x) # [:,1]\n",
    "        psx[cv_holdout_idx] = psx_cv\n",
    "\n",
    "        # Compute and append the confident counts noise estimators\n",
    "        # to estimate the positive and negative mislabeling rates.\n",
    "        joint_count_cv = estimate_joint_counts_from_probabilities(\n",
    "            s = s_holdout_cv, \n",
    "            psx = psx_cv, # P(s = k|x)\n",
    "            thresholds = thresholds, \n",
    "            force_ps = force_ps,\n",
    "            return_list_of_converging_jc_matrices = return_list_of_converging_jc_matrices,\n",
    "        )\n",
    "#         py_cv, noise_matrix_cv, inv_noise_matrix_cv, joint_count_cv = \\\n",
    "#         estimate_py_and_noise_matrices_from_probabilities(\n",
    "#             s = s_holdout_cv, \n",
    "#             psx = psx_cv, # P(s = k|x) \n",
    "#             thresholds = thresholds,\n",
    "#             converge_latent_estimates = False, # Converge at end, if we converge.\n",
    "#         )\n",
    "\n",
    "#         py_per_cv_fold.append(py_cv)\n",
    "#         noise_matrix_per_cv_fold.append(noise_matrix_cv)\n",
    "#         inv_noise_matrix_per_cv_fold.append(inv_noise_matrix_cv)\n",
    "        joint_count_per_cv_fold.append(joint_count_cv)\n",
    "    \n",
    "    if return_list_of_converging_jc_matrices:\n",
    "        jcs = [np.sum(np.stack(jcs), axis=0) for jcs in zip(*joint_count_per_cv_fold)] \n",
    "        return jcs, psx\n",
    "\n",
    "    \n",
    "# CONSIDER REMOVING THE FOLLOWING CODE WHICH IS NOW OBSELETE\n",
    "\n",
    "    # Compute mean py, noise_matrix, inverse noise marix (disregarding nan\n",
    "    #   or inf values) and psx\n",
    "    # Mean is computed by stacking each cv fold's noise_matrix (forming \n",
    "    #  a 3D tensor), then average along the newly formed depth axis to\n",
    "    #  to yield the mean 2D tensor.\n",
    "#     py = np.apply_along_axis(func1d=_mean_without_nan_inf, axis=0, arr=np.vstack(py_per_cv_fold))\n",
    "#     noise_matrix = np.apply_along_axis(func1d=_mean_without_nan_inf, axis=2, arr=np.dstack(noise_matrix_per_cv_fold))\n",
    "#     inv_noise_matrix = np.apply_along_axis(func1d=_mean_without_nan_inf, axis=2, arr=np.dstack(inv_noise_matrix_per_cv_fold))\n",
    "#     joint_count = np.apply_along_axis(func1d=_mean_without_nan_inf, axis=2, arr=np.dstack(joint_count_per_cv_fold)) \n",
    "\n",
    "#     if converge_latent_estimates: # Force numerical consistency of estimates.\n",
    "#         py, noise_matrix, inv_noise_matrix = converge_estimates(ps, py, noise_matrix, inv_noise_matrix)\n",
    "  \n",
    "#     return py, noise_matrix, inv_noise_matrix, psx\n",
    "\n",
    "    joint_count = np.sum(np.stack(joint_count_per_cv_fold), axis=0)\n",
    "    return joint_count, psx\n",
    "\n",
    "\n",
    "def compute_py_noise_matrices_and_cv_pred_proba(\n",
    "    X, \n",
    "    s, \n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    thresholds = None,\n",
    "    converge_latent_estimates = False,\n",
    "    force_ps = False,\n",
    "    return_list_of_converging_jc_matrices = False,\n",
    "    py_method = 'cnt',\n",
    "    seed = None,\n",
    "):\n",
    "    '''This function computes the out-of-sample predicted \n",
    "    probability P(s=k|x) for every example x in X using cross\n",
    "    validation while also computing the confident counts noise\n",
    "    rates within each cross-validated subset and returning\n",
    "    the average noise rate across all examples. \n",
    "\n",
    "    This function estimates the noise_matrix of shape (K, K). This is the\n",
    "    fraction of examples in every class, labeled as every other class. The\n",
    "    noise_matrix is a conditional probability matrix for P(s=k_s|y=k_y).\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent.\n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output joint_count matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the joint_count, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    return_list_of_converging_jc_matrices : bool (default = False)\n",
    "        When force_ps is true, it converges the joint count matrix that is returned.\n",
    "        Setting this to true will return the list of the converged matrices. The first\n",
    "        item in the list is the original and the last item is the final result.\n",
    "        \n",
    "    py_method : str\n",
    "        How to compute the latent prior p(y=k). Default is \"cnt\" as it tends to\n",
    "        work best, but you may also set this hyperparameter to \"eqn\" or \"marginal\".\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "      Returns a tuple of three numpy array matrices in the form:\n",
    "      (noise_matrix, inverse_noise_matrix, predicted probability matrix)'''\n",
    "    \n",
    "    joint_count, psx = estimate_joint_counts_and_cv_pred_proba(\n",
    "        X = X, \n",
    "        s = s, \n",
    "        clf = clf,\n",
    "        cv_n_folds = cv_n_folds,\n",
    "        thresholds = thresholds,\n",
    "        force_ps = force_ps,\n",
    "        return_list_of_converging_jc_matrices = return_list_of_converging_jc_matrices,\n",
    "        seed = seed,\n",
    "    )\n",
    "    \n",
    "    py, noise_matrix, inv_noise_matrix = estimate_latent(\n",
    "        joint_count = joint_count, \n",
    "        s = s, \n",
    "        py_method = py_method, \n",
    "        converge_latent_estimates = converge_latent_estimates,\n",
    "    )\n",
    "    \n",
    "    return py, noise_matrix, inv_noise_matrix, psx \n",
    "\n",
    "\n",
    "def compute_cv_predicted_probabilities(\n",
    "    X, \n",
    "    labels, # class labels can be noisy (s) or not noisy (y).\n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    seed = None,\n",
    "):\n",
    "    '''This function computes the out-of-sample predicted \n",
    "    probability [P(s=k|x)] for every example in X using cross\n",
    "    validation. Output is a np.array of shape (N, K) where N is \n",
    "    the number of training examples and K is the number of classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    labels : np.array or list of ints from [0,1,..,K-1]\n",
    "      A discrete vector of class labels which may or may not contain mislabeling\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.\n",
    "    '''\n",
    "\n",
    "    return compute_py_noise_matrices_and_cv_pred_proba(\n",
    "        X = X, \n",
    "        s = labels, \n",
    "        clf = clf,\n",
    "        cv_n_folds = cv_n_folds,\n",
    "        seed = seed,\n",
    "    )[-1]\n",
    "\n",
    "\n",
    "def compute_noise_matrices(\n",
    "    X, \n",
    "    s, \n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    thresholds = None,\n",
    "    converge_latent_estimates = True,\n",
    "    seed = None,\n",
    "):\n",
    "    '''Estimates the noise_matrix of shape (K, K). This is the\n",
    "    fraction of examples in every class, labeled as every other class. The\n",
    "    noise_matrix is a conditional probability matrix for P(s=k_s|y=k_y).\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent.\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.'''\n",
    "\n",
    "    return compute_py_noise_matrices_and_cv_pred_proba(\n",
    "        X = X, \n",
    "        s = s, \n",
    "        clf = clf,\n",
    "        cv_n_folds = cv_n_folds,\n",
    "        thresholds = thresholds,\n",
    "        converge_latent_estimates = converge_latent_estimates,\n",
    "        seed = seed,\n",
    "    )[1:-1]\n",
    "\n",
    "\n",
    "def compute_ps_py_inv_noise_matrix(s, noise_matrix):\n",
    "    '''Compute ps := P(s=k), py := P(y=k), and the inverse noise matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "        A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "        the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "        the fraction of examples in every class, labeled as every other class.\n",
    "        Assumes columns of noise_matrix sum to 1.'''\n",
    "  \n",
    "    # 'ps' is p(s=k)\n",
    "    ps = value_counts(s) / float(len(s))\n",
    "\n",
    "    py, inverse_noise_matrix = compute_py_inv_noise_matrix(ps, noise_matrix)\n",
    "    return ps, py, inverse_noise_matrix\n",
    "\n",
    "\n",
    "def compute_py_inv_noise_matrix(ps, noise_matrix):\n",
    "    '''Compute py := P(y=k), and the inverse noise matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ps : np.array (shape (K, 1))\n",
    "        The fraction (prior probability) of each observed, noisy class label, P(s = k)\n",
    "\n",
    "    noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "        the fraction of examples in every class, labeled as every other class.\n",
    "        Assumes columns of noise_matrix sum to 1.'''\n",
    "  \n",
    "    # Number of classes\n",
    "    K = len(ps)\n",
    "\n",
    "    # 'py' is p(y=k) = noise_matrix^(-1) * p(s=k)\n",
    "    # because in *vector computation*: P(s=k|y=k) * p(y=k) = P(s=k)\n",
    "    # The pseudoinverse is used when noise_matrix is not invertible.\n",
    "    py = np.linalg.inv(noise_matrix).dot(ps)\n",
    "\n",
    "    # No class should have probability 0 so we use .001\n",
    "    # Make sure valid probabilites that sum to 1.0\n",
    "    py = clip_values(py, low=0.001, high=1.0, new_sum = 1.0)\n",
    "\n",
    "    # All the work is done in this function (below)\n",
    "    return py, compute_inv_noise_matrix(py, noise_matrix, ps)\n",
    "\n",
    "\n",
    "def compute_inv_noise_matrix(py, noise_matrix, ps = None):\n",
    "  '''Compute the inverse noise matrix if py := P(y=k) is given.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "\n",
    "  py : np.array (shape (K, 1))\n",
    "    The fraction (prior probability) of each true, hidden class label, P(y = k)\n",
    "    \n",
    "  noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "    the fraction of examples in every class, labeled as every other class.\n",
    "    Assumes columns of noise_matrix sum to 1.\n",
    "\n",
    "  ps : np.array (shape (K, 1))\n",
    "    The fraction (prior probability) of each observed, noisy class label, P(s = k).\n",
    "    ps is easily computable from py and should only be provided if it has\n",
    "    already been precomputed, to increase code efficiency.'''\n",
    "  \n",
    "  # Number of classes\n",
    "  K = len(py)\n",
    "  \n",
    "  # 'ps' is p(s=k) = noise_matrix * p(s=k)\n",
    "  # because in *vector computation*: P(s=k|y=k) * p(y=k) = P(s=k)\n",
    "  if ps is None:\n",
    "    ps = noise_matrix.dot(py)\n",
    "  \n",
    "  # Estimate the (K, K) inverse noise matrix P(y = k_y | s = k_s)\n",
    "  inverse_noise_matrix = np.empty(shape=(K,K))\n",
    "  # k_s is the class value k of noisy label s\n",
    "  for k_s in range(K):\n",
    "      # k_y is the (guessed) class value k of true label y\n",
    "      for k_y in range(K):\n",
    "        # P(y|s) = P(s|y) * P(y) / P(s)\n",
    "        inverse_noise_matrix[k_y][k_s] = noise_matrix[k_s][k_y] * py[k_y] / ps[k_s]\n",
    "  \n",
    "  # Clip inverse noise rates P(y=k_s|y=k_y) into proper range [0,1)\n",
    "  return clip_noise_rates(inverse_noise_matrix)\n",
    "\n",
    "\n",
    "def compute_noise_matrix_from_inverse(ps, inverse_noise_matrix, py = None):\n",
    "  '''Compute the noise matrix P(s=k_s|y=k_y).\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "\n",
    "  py : np.array (shape (K, 1))\n",
    "    The fraction (prior probability) of each true, hidden class label, P(y = k)\n",
    "    \n",
    "  noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "    the fraction of examples in every class, labeled as every other class.\n",
    "    Assumes columns of noise_matrix sum to 1.\n",
    "\n",
    "  ps : np.array (shape (K, 1))\n",
    "    The fraction (prior probability) of each observed, noisy class label, P(s = k).\n",
    "    ps is easily computable from py and should only be provided if it has\n",
    "    already been precomputed, to increase code efficiency.\n",
    "    \n",
    "  Output\n",
    "  ------\n",
    "  \n",
    "  noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "    the fraction of examples in every class, labeled as every other class.\n",
    "    Columns of noise_matrix sum to 1.'''\n",
    "  \n",
    "  # Number of classes s\n",
    "  K = len(ps)\n",
    "  \n",
    "  # 'py' is p(y=k) = inverse_noise_matrix * p(y=k)\n",
    "  # because in *vector computation*: P(y=k|s=k) * p(s=k) = P(y=k)\n",
    "  if py is None:\n",
    "    py = inverse_noise_matrix.dot(ps)\n",
    "  \n",
    "  # Estimate the (K, K) noise matrix P(s = k_s | y = k_y)\n",
    "  noise_matrix = np.empty(shape=(K,K))\n",
    "  # k_s is the class value k of noisy label s\n",
    "  for k_s in range(K):\n",
    "      # k_y is the (guessed) class value k of true label y\n",
    "      for k_y in range(K):\n",
    "        # P(s|y) = P(y|s) * P(s) / P(y)\n",
    "        noise_matrix[k_s][k_y] = inverse_noise_matrix[k_y][k_s] * ps[k_s] / py[k_y]\n",
    "  \n",
    "  # Clip inverse noise rates P(y=k_y|y=k_s) into proper range [0,1)\n",
    "  return clip_noise_rates(noise_matrix)\n",
    "\n",
    "  \n",
    "def compute_py(ps, noise_matrix, inverse_noise_matrix):\n",
    "  '''Compute py := P(y=k) from ps := P(s=k), noise_matrix, and inverse noise matrix.\n",
    "  \n",
    "  This method is ** ROBUST ** - meaning it works well even when the\n",
    "  noise matrices are estimated poorly by only using the diagonals of the matrices\n",
    "  instead of all the probabilities in the entire matrix.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "\n",
    "  ps : np.array (shape (K, ) or (1, K)) \n",
    "    The fraction (prior probability) of each observed, noisy class label, P(s = k).\n",
    "    \n",
    "  noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "    the fraction of examples in every class, labeled as every other class.\n",
    "    Assumes columns of noise_matrix sum to 1.\n",
    "    \n",
    "  inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "    the estimated fraction observed examples in each class k_s, that are\n",
    "    mislabeled examples from every other class k_y. If None, the \n",
    "    inverse_noise_matrix will be computed from psx and s.\n",
    "    Assumes columns of inverse_noise_matrix sum to 1.\n",
    "    \n",
    "  Output\n",
    "  ------\n",
    "  \n",
    "  py : np.array (shape (K, ) or (1, K))\n",
    "    The fraction (prior probability) of each observed, noisy class label, P(y = k).'''\n",
    "  \n",
    "  if len(np.shape(ps)) > 2 or (len(np.shape(ps)) == 2 and np.shape(ps)[0] != 1):\n",
    "    import warnings\n",
    "    warnings.warn(\"Input parameter np.array 'ps' has shape \" + str(np.shape(ps)) + \\\n",
    "                  \", but shape should be (K, ) or (1, K)\")\n",
    "  \n",
    "  # Computing py this way avoids dividing by zero noise rates! Also more robust.\n",
    "  # More robust because error est_p(y|s) / est_p(s|y) ~ p(y|s) / p(s|y) \n",
    "  py = ps * inverse_noise_matrix.diagonal() / noise_matrix.diagonal()\n",
    "  # Make sure valid probabilites that sum to 1.0\n",
    "  return clip_values(py, low=0.0, high=1.0, new_sum = 1.0)\n",
    "  \n",
    "  \n",
    "\n",
    "def compute_pyx(psx, noise_matrix, inverse_noise_matrix):\n",
    "  '''Compute pyx := P(y=k|x) from psx := P(s=k|x), and the noise_matrix and inverse\n",
    "  noise matrix. \n",
    "  \n",
    "  This method is ROBUST - meaning it works well even when the\n",
    "  noise matrices are estimated poorly by only using the diagonals of the matrices\n",
    "  which tend to be easy to estimate correctly.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  psx : np.array (shape (N, K))\n",
    "    P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "    \n",
    "  noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "    the fraction of examples in every class, labeled as every other class.\n",
    "    Assumes columns of noise_matrix sum to 1.\n",
    "    \n",
    "  inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "    the estimated fraction observed examples in each class k_s, that are\n",
    "    mislabeled examples from every other class k_y. If None, the \n",
    "    inverse_noise_matrix will be computed from psx and s.\n",
    "    Assumes columns of inverse_noise_matrix sum to 1.\n",
    "    \n",
    "  Output\n",
    "  ------\n",
    "  \n",
    "  pyx : np.array (shape (N, K))\n",
    "    P(y=k|x) is a matrix with K probabilities for each of the N examples x.'''\n",
    "  \n",
    "  if len(np.shape(psx)) != 2:\n",
    "    raise ValueError(\"Input parameter np.array 'psx' has shape \" + str(np.shape(psx)) + \\\n",
    "                  \", but shape should be (N, K)\")\n",
    "  \n",
    "  pyx = psx * inverse_noise_matrix.diagonal() / noise_matrix.diagonal()\n",
    "  # Make sure valid probabilites that sum to 1.0\n",
    "  return np.apply_along_axis(\n",
    "    func1d=clip_values, \n",
    "    axis=1, \n",
    "    arr=pyx,\n",
    "    **{\"low\":0.0, \"high\":1.0, \"new_sum\":1.0}\n",
    "  )\n",
    "  \n",
    "\n",
    "def converge_estimates(\n",
    "    ps,\n",
    "    py,\n",
    "    noise_matrix, \n",
    "    inverse_noise_matrix, \n",
    "    inv_noise_matrix_iterations = 5,\n",
    "    noise_matrix_iterations = 3,\n",
    "):\n",
    "    '''Computes py := P(y=k) and both noise_matrix and inverse_noise_matrix,\n",
    "    by numerically converging ps := P(s=k), py, and the noise matrices.\n",
    "\n",
    "    Forces numerical consistency of estimates. Each is estimated\n",
    "    independently, but they are related mathematically with closed form \n",
    "    equivalences. This will iteratively make them mathematically consistent. \n",
    "\n",
    "    py := P(y=k) and the inverse noise matrix P(y=k_y|s=k_s) specify one another, \n",
    "    meaning one can be computed from the other and vice versa. When numerical\n",
    "    discrepancy exists due to poor estimation, they can be made to agree by repeatedly\n",
    "    computing one from the other, for some a certain number of iterations (3-10 works fine.)\n",
    "\n",
    "    Do not set iterations too high or performance will decrease as small deviations\n",
    "    will get perturbated over and over and potentially magnified.\n",
    "\n",
    "    Note that we have to first converge the inverse_noise_matrix and py, \n",
    "    then we can update the noise_matrix, then repeat. This is becauase the\n",
    "    inverse noise matrix depends on py (which is unknown/latent), but the\n",
    "    noise matrix depends on ps (which is known), so there will be no change\n",
    "    in the noise matrix if we recompute it when py and inverse_noise_matrix change.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ps : np.array (shape (K, ) or (1, K))\n",
    "        The fraction (prior probability) of each observed, noisy class label, P(y = k).\n",
    "\n",
    "    noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "        the fraction of examples in every class, labeled as every other class.\n",
    "        Assumes columns of noise_matrix sum to 1.\n",
    "\n",
    "    inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "        the estimated fraction observed examples in each class k_s, that are\n",
    "        mislabeled examples from every other class k_y. If None, the \n",
    "        inverse_noise_matrix will be computed from psx and s.\n",
    "        Assumes columns of inverse_noise_matrix sum to 1.\n",
    "\n",
    "    Output\n",
    "    ------  \n",
    "        Three np.arrays of the form (py, noise_matrix, inverse_noise_matrix) with py \n",
    "        and inverse_noise_matrix and noise_matrix having numerical agreement.'''  \n",
    "  \n",
    "    for j in range(noise_matrix_iterations):\n",
    "        for i in range(inv_noise_matrix_iterations):\n",
    "            inverse_noise_matrix = compute_inv_noise_matrix(py, noise_matrix, ps)\n",
    "            py = compute_py(ps, noise_matrix, inverse_noise_matrix)\n",
    "        noise_matrix = compute_noise_matrix_from_inverse(ps, inverse_noise_matrix, py)\n",
    "    \n",
    "    return py, noise_matrix, inverse_noise_matrix\n",
    "  \n",
    "  \n",
    "def get_noise_indices(\n",
    "    s, \n",
    "    psx, \n",
    "    inverse_noise_matrix = None,\n",
    "    frac_noise = 1.0,\n",
    "    num_to_remove_per_class = None,\n",
    "    method = 'prune_by_noise_rate',\n",
    "    joint_count = None,\n",
    "):\n",
    "    '''Returns the indices of most likely (confident) label errors in s. The\n",
    "    number of indices returned is specified by frac_of_noise. When \n",
    "    frac_of_noise = 1.0, all \"confidently\" estimated noise indices are returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "    \n",
    "    psx : np.array (shape (N, K))\n",
    "      P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "      This is the probability distribution over all K classes, for each\n",
    "      example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "      have been computed using 3 (or higher) fold cross-validation.\n",
    "      \n",
    "    inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "      A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "      the estimated fraction observed examples in each class k_s, that are\n",
    "      mislabeled examples from every other class k_y. If None, the \n",
    "      inverse_noise_matrix will be computed from psx and s.\n",
    "      Assumes columns of inverse_noise_matrix sum to 1.\n",
    "  \n",
    "    frac_noise : float\n",
    "      When frac_of_noise = 1.0, return all \"confidently\" estimated noise indices.\n",
    "      Value in range (0, 1] that determines the fraction of noisy example \n",
    "      indices to return based on the following formula for example class k.\n",
    "      frac_of_noise * number_of_mislabeled_examples_in_class_k, or equivalently    \n",
    "      frac_of_noise * inverse_noise_rate_class_k * num_examples_with_s_equal_k\n",
    "      \n",
    "    num_to_remove_per_class : list of int of length K (# of classes)\n",
    "      e.g. if K = 3, num_to_remove_per_class = [5, 0, 1] would return \n",
    "      the indices of the 5 most likely mislabeled examples in class s = 0,\n",
    "      and the most likely mislabeled example in class s = 1.\n",
    "      ***Only set this parameter if method == 'prune_by_class'\n",
    "      \n",
    "    method : str (default: 'prune_by_noise_rate')\n",
    "      'prune_by_class', 'prune_by_noise_rate', or 'both'. Method used for pruning.\n",
    "      'both' creates a mask based on removing the least likely in\n",
    "      each class (prune_by_class) and a mask based on the most likely to be labeled\n",
    "      another class (prune_by_noise_rate) and then 'AND's the two masks disjunctively\n",
    "      such that an example must be pruned under both masks to be pruned.\n",
    "        \n",
    "    joint_count : np.array (shape (K, K), type int) (default: None)\n",
    "        NOTE: If this is provided, pruning counts will be determined entirely by\n",
    "        this input and the inverse_noise_matrix input WILL BE IGNORED!\n",
    "        A K,K integer matrix of count(s=k, y=k). Captures the joint disribution of\n",
    "        the noisy and true labels P_{s,y}. Each entry in the matrix contains\n",
    "        the number of examples confidently counted into both classes.'''\n",
    "  \n",
    "    # Number of examples in each class of s\n",
    "    s_counts = value_counts(s)\n",
    "    # 'ps' is p(s=k)\n",
    "    ps = s_counts / float(len(s))\n",
    "    # Number of classes s\n",
    "    K = len(psx.T)\n",
    "\n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "\n",
    "    if inverse_noise_matrix is None:\n",
    "        py, noise_matrix, inverse_noise_matrix, _ = \\\n",
    "        estimate_py_and_noise_matrices_from_probabilities(s, psx, converge_latent_estimates=converge_latent_estimates)\n",
    "  \n",
    "    # Estimate the number of examples to confidently prune per class.\n",
    "    if joint_count is None:\n",
    "        prune_count_matrix = inverse_noise_matrix * s_counts # Matrix of counts(y=k and s=l)\n",
    "    else:\n",
    "        prune_count_matrix = joint_count.T / float(joint_count.sum()) * len(s) # calibrate\n",
    "    # Leave at least MIN_NUM_PER_CLASS examples per class.\n",
    "    prune_count_matrix = keep_at_least_n_per_class(\n",
    "        prune_count_matrix=prune_count_matrix, \n",
    "        n=MIN_NUM_PER_CLASS, \n",
    "        frac_noise=frac_noise,\n",
    "    )\n",
    "  \n",
    "    if num_to_remove_per_class is not None:\n",
    "        np.fill_diagonal(prune_count_matrix, s_counts - num_to_remove_per_class)\n",
    "\n",
    "    # Initialize the boolean mask of noise indices.\n",
    "    noise_mask = np.zeros(len(psx), dtype=bool)\n",
    "\n",
    "    # Peform Pruning with threshold probabilities from BFPRT algorithm in O(n)\n",
    "\n",
    "    if method == 'prune_by_class' or method == 'both':\n",
    "        for k in range(K):\n",
    "            if s_counts[k] > MIN_NUM_PER_CLASS: # Don't prune if not MIN_NUM_PER_CLASS\n",
    "                num2prune = s_counts[k] - prune_count_matrix[k][k]\n",
    "                # num2keep'th smallest probability of class k for examples with noisy label k\n",
    "                threshold = np.partition(psx[:,k][s == k], num2prune)[num2prune]\n",
    "                noise_mask = noise_mask | ((psx[:,k] < threshold) & (s == k))\n",
    "  \n",
    "    if method == 'both':\n",
    "        noise_mask_by_class = noise_mask\n",
    "\n",
    "    if method == 'prune_by_noise_rate' or method == 'both':\n",
    "        noise_mask = np.zeros(len(psx), dtype=bool)\n",
    "        for k in range(K):\n",
    "            if s_counts[k] > MIN_NUM_PER_CLASS: # Don't prune if not MIN_NUM_PER_CLASS\n",
    "                for j in range(K):\n",
    "                    if k!=j: # Only prune for noise rates\n",
    "                        num2prune = prune_count_matrix[k][j]\n",
    "                        # num2prune'th largest probability of class k for examples with noisy label j\n",
    "                        threshold = -np.partition(-psx[:,k][s == j], num2prune)[num2prune]\n",
    "                        noise_mask = noise_mask | ((psx[:,k] > threshold) & (s == j))\n",
    "            \n",
    "    return noise_mask & noise_mask_by_class if method == 'both' else noise_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generic helper function`s exposed to rankpruning module.   \n",
    "\n",
    "def keep_at_least_n_per_class(prune_count_matrix, n, frac_noise=1.0):\n",
    "  '''Make sure every class has at least n examples after removing noise.\n",
    "  Functionally, increase each column, increases the diagonal term #(y=k,s=k) of \n",
    "  prune_count_matrix until it is at least n, distributing the amount increased\n",
    "  by subtracting uniformly from the rest of the terms in the column. When \n",
    "  frac_of_noise = 1.0, return all \"confidently\" estimated noise indices, otherwise\n",
    "  this returns frac_of_noise fraction of all the noise counts, with diagonal terms\n",
    "  adjusted to ensure column totals are preserved.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "    \n",
    "  prune_count_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A counts of mislabeled examples in every class. For this function, it\n",
    "    does not matter what the rows or columns are, but the diagonal terms\n",
    "    reflect the number of correctly labeled examples.\n",
    "    \n",
    "  n : int\n",
    "    Number of examples to make sure are left in each class.\n",
    "\n",
    "  frac_noise : float\n",
    "    When frac_of_noise = 1.0, return all \"confidently\" estimated noise indices.\n",
    "    Value in range (0, 1] that determines the fraction of noisy example \n",
    "    indices to return based on the following formula for example class k.\n",
    "    frac_of_noise * number_of_mislabeled_examples_in_class_k, or equivalently    \n",
    "    frac_of_noise * inverse_noise_rate_class_k * num_examples_with_s_equal_k\n",
    "    \n",
    "  Output\n",
    "  ------\n",
    "  \n",
    "  prune_count_matrix : np.array of shape (K, K), K = number of classes \n",
    "    Number of examples to remove from each class, for every other class.'''\n",
    "  \n",
    "  K = len(prune_count_matrix)\n",
    "  prune_count_matrix_diagonal = np.diagonal(prune_count_matrix)\n",
    "  \n",
    "  # Set diagonal terms less than n, to n. \n",
    "  new_diagonal = np.maximum(prune_count_matrix_diagonal, n)\n",
    "\n",
    "  # Find how much diagonal terms were increased.\n",
    "  diff_per_col = new_diagonal - prune_count_matrix_diagonal\n",
    "\n",
    "  # Count non-zero, non-diagonal items per column\n",
    "  # np.maximum(*, 1) makes this never 0 (we divide by this next)\n",
    "  num_noise_rates_per_col = np.maximum(np.count_nonzero(prune_count_matrix, axis=0) - 1., 1.) \n",
    "\n",
    "  # Uniformly decrease non-zero noise rates by amount diagonal items were increased\n",
    "  new_mat = prune_count_matrix - diff_per_col / num_noise_rates_per_col\n",
    "  \n",
    "  # Originally zero noise rates will now be negative, fix them back to zero\n",
    "  new_mat[new_mat < 0] = 0\n",
    "\n",
    "  # Round diagonal terms (correctly labeled examples)\n",
    "  np.fill_diagonal(new_mat, new_diagonal.round())\n",
    "  \n",
    "  # Reduce (multiply) all noise rates (non-diagonal) by frac_noise and\n",
    "  # increase diagonal by the total amount reduced in each column to preserve column counts.\n",
    "  new_mat = reduce_prune_counts(new_mat, frac_noise)\n",
    "  \n",
    "  # These are counts, so return a matrix of ints.\n",
    "  return new_mat.astype(int)\n",
    "  \n",
    "\n",
    "def reduce_prune_counts(prune_count_matrix, frac_noise=1.0):\n",
    "  '''Reduce (multiply) all prune counts (non-diagonal) by frac_noise and\n",
    "  increase diagonal by the total amount reduced in each column to \n",
    "  preserve column counts.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "    \n",
    "  prune_count_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A counts of mislabeled examples in every class. For this function, it\n",
    "    does not matter what the rows or columns are, but the diagonal terms\n",
    "    reflect the number of correctly labeled examples.\n",
    "\n",
    "  frac_noise : float\n",
    "    When frac_of_noise = 1.0, return all \"confidently\" estimated noise indices.\n",
    "    Value in range (0, 1] that determines the fraction of noisy example \n",
    "    indices to return based on the following formula for example class k.\n",
    "    frac_of_noise * number_of_mislabeled_examples_in_class_k, or equivalently    \n",
    "    frac_of_noise * inverse_noise_rate_class_k * num_examples_with_s_equal_k'''\n",
    "  \n",
    "  new_mat = prune_count_matrix * frac_noise\n",
    "  np.fill_diagonal(new_mat, prune_count_matrix.diagonal())\n",
    "  np.fill_diagonal(new_mat, prune_count_matrix.diagonal() + np.sum(prune_count_matrix - new_mat, axis=0))\n",
    "  \n",
    "  # These are counts, so return a matrix of ints.\n",
    "  return new_mat.astype(int)\n",
    "  \n",
    "\n",
    "def clip_noise_rates(noise_matrix):\n",
    "  '''Clip all noise rates to proper range [0,1), but\n",
    "  do not modify the diagonal terms because they are not\n",
    "  noise rates.\n",
    "  \n",
    "  ASSUMES noise_matrix columns sum to 1.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "    \n",
    "  noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix containing the fraction of\n",
    "    examples in every class, labeled as every other class.\n",
    "    Diagonal terms are not noise rates, but are consistency P(s=k|y=k)\n",
    "    Assumes columns of noise_matrix sum to 1'''\n",
    "  \n",
    "  def clip_noise_rate_range(noise_rate):\n",
    "    '''Clip noise rate P(s=k'|y=k) or P(y=k|s=k')\n",
    "    into proper range [0,1)'''\n",
    "    return min(max(noise_rate, 0.0), 0.9999)\n",
    "  \n",
    "  # Vectorize clip_noise_rate_range for efficiency with np.arrays.  \n",
    "  vectorized_clip = np.vectorize(clip_noise_rate_range)\n",
    "  \n",
    "  # Preserve because diagonal entries are not noise rates.\n",
    "  diagonal = np.diagonal(noise_matrix)\n",
    "  \n",
    "  # Clip all noise rates (efficiently).\n",
    "  noise_matrix = vectorized_clip(noise_matrix)\n",
    "  \n",
    "  # Put unmodified diagonal back.\n",
    "  np.fill_diagonal(noise_matrix, diagonal)\n",
    "  \n",
    "  # Re-normalized noise_matrix so that columns sum to one.\n",
    "  noise_matrix = noise_matrix / noise_matrix.sum(axis=0)\n",
    "  \n",
    "  return noise_matrix\n",
    "\n",
    "\n",
    "def clip_values(x, low = 0.0, high = 1.0, new_sum = None):\n",
    "  '''Clip all values in p to range [low,high].\n",
    "  Preserves sum of x.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "    \n",
    "  x : np.array \n",
    "    An array / list of values to be clipped.\n",
    "    \n",
    "  low : float\n",
    "    values in x greater than 'low' are clipped to this value\n",
    "    \n",
    "  high : float\n",
    "    values in x greater than 'high' are clipped to this value\n",
    "    \n",
    "  new_sum : float\n",
    "    normalizes x after clipping to sum to new_sum\n",
    "    \n",
    "  Returns\n",
    "  -------\n",
    "  \n",
    "  x : np.array\n",
    "    A list of clipped values, summing to the same sum as x.'''\n",
    "  \n",
    "  def clip_range(a, low = low, high = high):\n",
    "    '''Clip a into range [low,high]'''\n",
    "    return min(max(a, low), high)\n",
    "  \n",
    "  # Vectorize clip_range for efficiency with np.arrays.  \n",
    "  vectorized_clip = np.vectorize(clip_range)\n",
    "  \n",
    "  # Store previous sum\n",
    "  prev_sum = sum(x) if new_sum is None else new_sum\n",
    "  \n",
    "  # Clip all values (efficiently).\n",
    "  x = vectorized_clip(x)\n",
    "  \n",
    "  # Re-normalized values to sum to previous sum.\n",
    "  x = x * prev_sum / float(sum(x))\n",
    "  \n",
    "  return x\n",
    "\n",
    "\n",
    "def value_counts(x):\n",
    "  '''Returns an np.array of shape (K, 1), with the\n",
    "  value counts for every unique item in the labels list/array, \n",
    "  where K is the number of unique entries in labels.\n",
    "  \n",
    "  Why this matters? Here is an example:\n",
    "    x = [np.random.randint(0,100) for i in range(100000)]\n",
    "\n",
    "    %timeit np.bincount(x)\n",
    "    --Result: 100 loops, best of 3: 3.9 ms per loop\n",
    "\n",
    "    %timeit np.unique(x, return_counts=True)[1]\n",
    "    --Result: 100 loops, best of 3: 7.47 ms per loop\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  x : list or np.array (one dimensional)\n",
    "    A list of discrete objects, like lists or strings, for\n",
    "    example, class labels 'y' when training a classifier.\n",
    "    e.g. [\"dog\",\"dog\",\"cat\"] or [1,2,0,1,1,0,2]\n",
    "'''\n",
    "  if type(x[0]) is int and (np.array(x) >= 0).all():\n",
    "    return np.bincount(x)\n",
    "  else:\n",
    "    return np.unique(x, return_counts=True)[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Private generic helper functions.\n",
    "\n",
    "# def _mean_without_nan_inf(arr, replacement = None):\n",
    "#   '''Private helper method for computing the mean\n",
    "#   of a 1D numpy array or iterable by replacing NaN and inf\n",
    "#   values with a replacement value or ignore those values\n",
    "#   if replacement = None.\n",
    "\n",
    "#   Parameters \n",
    "#   ----------\n",
    "#   arr : iterable (list or 1D np.array)\n",
    "#     Any 1-dimensional iterable that may contain NaN or inf values.\n",
    "\n",
    "#   replacement : float\n",
    "#     Replace NaN and inf values in arr with this value.\n",
    "#   '''\n",
    "#   if replacement is not None:\n",
    "#     return np.mean(\n",
    "#       [replacement if math.isnan(x) or math.isinf(x) else x for x in arr]\n",
    "#     )\n",
    "  \n",
    "#   x_real = [x for x in arr if not math.isnan(x) and not math.isinf(x)]\n",
    "  \n",
    "#   if len(x_real) == 0:\n",
    "#       raise ValueError(\"All values are np.NaN or np.inf. If you are\" \\\n",
    "#         \"using this function in the context of Rank Pruning:\\n\\n\" \\\n",
    "#         \"\\tFor Rank Pruning: All noise_rate estimates are np.NaN or np.inf\" \\\n",
    "#         \"for one of the noise rates in the noise_matrix. Check that\" \\\n",
    "#         \"threshold values are not too extreme (near 1 or 0), \" \\\n",
    "#         \"resulting in division by zero.\")\n",
    "#   else:\n",
    "#     return np.mean(x_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful functions for multiclass learning with noisy labels\n",
    "\n",
    "def generate_noisy_labels(y, noise_matrix, verbose=False):  \n",
    "  '''Generates noisy labels s (shape (N, 1)) from perfect labels y,\n",
    "  'exactly' yielding the provided noise_matrix between s and y.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "\n",
    "  y : np.array (shape (N, 1))\n",
    "    Perfect labels, without any noise. Contains K distinct natural number\n",
    "    classes, e.g. 0, 1,..., K-1\n",
    "    \n",
    "  noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "    A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "    the fraction of examples in every class, labeled as every other class.\n",
    "    Assumes columns of noise_matrix sum to 1.'''\n",
    "  \n",
    "  # Number of classes\n",
    "  K = len(noise_matrix)\n",
    "  \n",
    "  # Compute p(y=k)\n",
    "  py = value_counts(y) / float(len(y))\n",
    "  \n",
    "  # Generate s\n",
    "  count_joint = (noise_matrix * py * len(y)).round().astype(int) # count(s and y)\n",
    "  s = np.array(y)\n",
    "  for k_s in range(K):\n",
    "    for k_y in range(K):\n",
    "      if k_s != k_y:\n",
    "        s[np.random.choice(np.where((s==k_y)&(y==k_y))[0], count_joint[k_s][k_y], replace=False)] = k_s\n",
    "\n",
    "  # Compute the actual noise matrix induced by s\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  counts = confusion_matrix(s, y).astype(float)\n",
    "  new_noise_matrix = counts / counts.sum(axis=0)\n",
    "\n",
    "  # Validate that s indeed produces the correct noise_matrix (or close to it)\n",
    "  if np.linalg.norm(noise_matrix - new_noise_matrix) > 1:\n",
    "    raise ValueError(\"s does not yield the same noise_matrix. \" + \\\n",
    "                    \"The difference in norms is \" + str(np.linalg.norm(noise_matrix - new_noise_matrix)))\n",
    "\n",
    "  return s  \n",
    "\n",
    "\n",
    "def estimate_pu_f1(s, prob_s_eq_1):\n",
    "  '''    Computes Claesen's estimate of f1 in the pulearning setting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s : iterable (list or np.array)\n",
    "      Binary label (whether each element is labeled or not) in pu learning.\n",
    "      \n",
    "    prob_s_eq_1 : iterable (list or np.array)\n",
    "      The probability, for each example, whether it is s==1 P(s==1|x)\n",
    "      \n",
    "    Output (float)\n",
    "    ------\n",
    "    Claesen's estimate for f1 in the pulearning setting.'''\n",
    "  \n",
    "  pred = prob_s_eq_1 >= 0.5\n",
    "  true_positives = sum((np.array(s) == 1) & (np.array(pred) == 1))\n",
    "  all_positives = sum(s)\n",
    "  recall = true_positives / float(all_positives)\n",
    "  frac_positive_predictions = sum(pred) / float(len(s))\n",
    "  return recall ** 2 / frac_positive_predictions if frac_positive_predictions != 0 else np.nan\n",
    "\n",
    "\n",
    "def compute_confusion_noise_rate_matrix(y, s):\n",
    "  '''Implements a confusion matrix assuming y as true classes\n",
    "  and s as noisy (or sometimes predicted) classes.\n",
    "  \n",
    "  Results are identical (and similar computation time) to: \n",
    "    \"from sklearn.metrics import confusion_matrix\"\n",
    "    \n",
    "  However, this function avoids the dependency on sklearn.'''\n",
    "  \n",
    "  K = len(np.unique(y)) # Number of classes \n",
    "  result = np.zeros((K, K))\n",
    "  \n",
    "  for i in range(len(y)):\n",
    "    result[y[i]][s[i]] += 1\n",
    "    \n",
    "  return result.astype(float) / result.sum(axis=0)  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
