{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "#### Contains methods for estimating the latent indices of all label errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, absolute_import, division, unicode_literals, with_statement\n",
    "import numpy as np\n",
    "\n",
    "from confidentlearning.util import value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave at least this many examples in each class after\n",
    "# pruning, regardless if noise estimates are larger.\n",
    "MIN_NUM_PER_CLASS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_indices(\n",
    "    s, \n",
    "    psx, \n",
    "    inverse_noise_matrix = None,\n",
    "    confident_joint = None,\n",
    "    frac_noise = 1.0,\n",
    "    num_to_remove_per_class = None,\n",
    "    prune_method = 'prune_by_noise_rate',\n",
    "    prune_count_method = 'inverse_nm_dot_s',\n",
    "    converge_latent_estimates = False,\n",
    "):\n",
    "    '''Returns the indices of most likely (confident) label errors in s. The\n",
    "    number of indices returned is specified by frac_of_noise. When \n",
    "    frac_of_noise = 1.0, all \"confidently\" estimated noise indices are returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "      A binary vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "    \n",
    "    psx : np.array (shape (N, K))\n",
    "      P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "      This is the probability distribution over all K classes, for each\n",
    "      example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "      have been computed using 3 (or higher) fold cross-validation.\n",
    "      \n",
    "    inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "      A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "      the estimated fraction observed examples in each class k_s, that are\n",
    "      mislabeled examples from every other class k_y. If None, the \n",
    "      inverse_noise_matrix will be computed from psx and s.\n",
    "      Assumes columns of inverse_noise_matrix sum to 1.\n",
    "        \n",
    "    confident_joint : np.array (shape (K, K), type int) (default: None)\n",
    "      A K,K integer matrix of count(s=k, y=k). Estimatesa a confident subset of\n",
    "      the joint disribution of the noisy and true labels P_{s,y}.\n",
    "      Each entry in the matrix contains the number of examples confidently\n",
    "      counted into every pair (s=j, y=k) classes.\n",
    "  \n",
    "    frac_noise : float\n",
    "      When frac_of_noise = 1.0, return all \"confidently\" estimated noise indices.\n",
    "      Value in range (0, 1] that determines the fraction of noisy example \n",
    "      indices to return based on the following formula for example class k.\n",
    "      frac_of_noise * number_of_mislabeled_examples_in_class_k, or equivalently    \n",
    "      frac_of_noise * inverse_noise_rate_class_k * num_examples_with_s_equal_k\n",
    "      \n",
    "    num_to_remove_per_class : list of int of length K (# of classes)\n",
    "      e.g. if K = 3, num_to_remove_per_class = [5, 0, 1] would return \n",
    "      the indices of the 5 most likely mislabeled examples in class s = 0,\n",
    "      and the most likely mislabeled example in class s = 1.\n",
    "      ***Only set this parameter if prune_method == 'prune_by_class'\n",
    "\n",
    "    prune_method : str (default: 'prune_by_noise_rate')\n",
    "      'prune_by_class', 'prune_by_noise_rate', or 'both'. Method used for pruning.\n",
    "      1. 'prune_by_noise_rate': works by removing examples with *high probability* of \n",
    "      being mislabeled for every non-diagonal in the prune_counts_matrix (see pruning.py).\n",
    "      2. 'prune_by_class': works by removing the examples with *smallest probability* of\n",
    "      belonging to their given class label for every class.\n",
    "      3. 'both': Finds the examples satisfying (1) AND (2) and removes their set conjunction. \n",
    "\n",
    "    prune_count_method : str (default 'inverse_nm_dot_s')\n",
    "      Options are 'inverse_nm_dot_s' or 'calibrate_confident_joint'. \n",
    "      Determines the method used to estimate the counts of the joint P(s, y) that will \n",
    "      be used to determine how many examples to prune\n",
    "      for every class that are flipped to every other class, as follows:\n",
    "        if prune_count_method == 'inverse_nm_dot_s':\n",
    "          prune_count_matrix = inverse_noise_matrix * s_counts # Matrix of counts(y=k and s=l)\n",
    "        elif prune_count_method == 'calibrate_confident_joint':# calibrate\n",
    "          prune_count_matrix = confident_joint.T / float(confident_joint.sum()) * len(s) \n",
    "\n",
    "    converge_latent_estimates : bool (Default: False)\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively enforce mathematically consistency.'''\n",
    "  \n",
    "    # Number of examples in each class of s\n",
    "    s_counts = value_counts(s)\n",
    "    # 'ps' is p(s=k)\n",
    "    ps = s_counts / float(len(s))\n",
    "    # Number of classes s\n",
    "    K = len(psx.T)\n",
    "\n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "    \n",
    "    # Estimate the number of examples to confidently prune for each (s=j, y=k) pair.\n",
    "    if (inverse_noise_matrix is None and prune_count_method == 'inverse_nm_dot_s') or \\\n",
    "       (confident_joint is None and prune_count_method == 'calibrate_confident_joint'):\n",
    "            from confidentlearning.latent_estimation import estimate_py_and_noise_matrices_from_probabilities\n",
    "            _, _, inverse_noise_matrix, confident_joint = estimate_py_and_noise_matrices_from_probabilities(\n",
    "                s, \n",
    "                psx, \n",
    "                converge_latent_estimates=converge_latent_estimates,\n",
    "            )\n",
    "    if prune_count_method == 'inverse_nm_dot_s':\n",
    "        prune_count_matrix = inverse_noise_matrix * s_counts # Matrix of counts(y=k and s=l)\n",
    "    elif prune_count_method == 'calibrate_confident_joint':\n",
    "        prune_count_matrix = confident_joint.T / float(confident_joint.sum()) * len(s) # calibrate\n",
    "    else:\n",
    "        raise ValueError(\"prune_count_method should be 'inverse_nm_dot_s' or \" + \n",
    "            \"'calibrate_confident_joint', but '\" + prune_count_method + \"' was given.\")\n",
    "        \n",
    "    # Leave at least MIN_NUM_PER_CLASS examples per class.\n",
    "    prune_count_matrix = keep_at_least_n_per_class(\n",
    "        prune_count_matrix=prune_count_matrix, \n",
    "        n=MIN_NUM_PER_CLASS, \n",
    "        frac_noise=frac_noise,\n",
    "    )\n",
    "  \n",
    "    if num_to_remove_per_class is not None:\n",
    "        np.fill_diagonal(prune_count_matrix, s_counts - num_to_remove_per_class)\n",
    "\n",
    "    # Initialize the boolean mask of noise indices.\n",
    "    noise_mask = np.zeros(len(psx), dtype=bool)\n",
    "\n",
    "    # Peform Pruning with threshold probabilities from BFPRT algorithm in O(n)\n",
    "\n",
    "    if prune_method == 'prune_by_class' or prune_method == 'both':\n",
    "        for k in range(K):\n",
    "            if s_counts[k] > MIN_NUM_PER_CLASS: # Don't prune if not MIN_NUM_PER_CLASS\n",
    "                num2prune = s_counts[k] - prune_count_matrix[k][k]\n",
    "                # num2keep'th smallest probability of class k for examples with noisy label k\n",
    "                threshold = np.partition(psx[:,k][s == k], num2prune)[num2prune]\n",
    "                noise_mask = noise_mask | ((psx[:,k] < threshold) & (s == k))\n",
    "  \n",
    "    if prune_method == 'both':\n",
    "        noise_mask_by_class = noise_mask\n",
    "\n",
    "    if prune_method == 'prune_by_noise_rate' or prune_method == 'both':\n",
    "        noise_mask = np.zeros(len(psx), dtype=bool)\n",
    "        for k in range(K):\n",
    "            if s_counts[k] > MIN_NUM_PER_CLASS: # Don't prune if not MIN_NUM_PER_CLASS\n",
    "                for j in range(K):\n",
    "                    if k!=j: # Only prune for noise rates\n",
    "                        num2prune = prune_count_matrix[k][j]\n",
    "                        # num2prune'th largest probability of class k for examples with noisy label j\n",
    "                        threshold = -np.partition(-psx[:,k][s == j], num2prune)[num2prune]\n",
    "                        noise_mask = noise_mask | ((psx[:,k] > threshold) & (s == j))\n",
    "            \n",
    "    return noise_mask & noise_mask_by_class if prune_method == 'both' else noise_mask\n",
    "\n",
    "\n",
    "def keep_at_least_n_per_class(prune_count_matrix, n, frac_noise=1.0):\n",
    "    '''Make sure every class has at least n examples after removing noise.\n",
    "    Functionally, increase each column, increases the diagonal term #(y=k,s=k) of \n",
    "    prune_count_matrix until it is at least n, distributing the amount increased\n",
    "    by subtracting uniformly from the rest of the terms in the column. When \n",
    "    frac_of_noise = 1.0, return all \"confidently\" estimated noise indices, otherwise\n",
    "    this returns frac_of_noise fraction of all the noise counts, with diagonal terms\n",
    "    adjusted to ensure column totals are preserved.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    prune_count_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A counts of mislabeled examples in every class. For this function, it\n",
    "        does not matter what the rows or columns are, but the diagonal terms\n",
    "        reflect the number of correctly labeled examples.\n",
    "\n",
    "    n : int\n",
    "        Number of examples to make sure are left in each class.\n",
    "\n",
    "    frac_noise : float\n",
    "        When frac_of_noise = 1.0, return all \"confidently\" estimated noise indices.\n",
    "        Value in range (0, 1] that determines the fraction of noisy example \n",
    "        indices to return based on the following formula for example class k.\n",
    "        frac_of_noise * number_of_mislabeled_examples_in_class_k, or equivalently    \n",
    "        frac_of_noise * inverse_noise_rate_class_k * num_examples_with_s_equal_k\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "  \n",
    "    prune_count_matrix : np.array of shape (K, K), K = number of classes \n",
    "        Number of examples to remove from each class, for every other class.'''\n",
    "  \n",
    "    K = len(prune_count_matrix)\n",
    "    prune_count_matrix_diagonal = np.diagonal(prune_count_matrix)\n",
    "\n",
    "    # Set diagonal terms less than n, to n. \n",
    "    new_diagonal = np.maximum(prune_count_matrix_diagonal, n)\n",
    "\n",
    "    # Find how much diagonal terms were increased.\n",
    "    diff_per_col = new_diagonal - prune_count_matrix_diagonal\n",
    "\n",
    "    # Count non-zero, non-diagonal items per column\n",
    "    # np.maximum(*, 1) makes this never 0 (we divide by this next)\n",
    "    num_noise_rates_per_col = np.maximum(np.count_nonzero(prune_count_matrix, axis=0) - 1., 1.) \n",
    "\n",
    "    # Uniformly decrease non-zero noise rates by amount diagonal items were increased\n",
    "    new_mat = prune_count_matrix - diff_per_col / num_noise_rates_per_col\n",
    "\n",
    "    # Originally zero noise rates will now be negative, fix them back to zero\n",
    "    new_mat[new_mat < 0] = 0\n",
    "\n",
    "    # Round diagonal terms (correctly labeled examples)\n",
    "    np.fill_diagonal(new_mat, new_diagonal.round())\n",
    "\n",
    "    # Reduce (multiply) all noise rates (non-diagonal) by frac_noise and\n",
    "    # increase diagonal by the total amount reduced in each column to preserve column counts.\n",
    "    new_mat = reduce_prune_counts(new_mat, frac_noise)\n",
    "\n",
    "    # These are counts, so return a matrix of ints.\n",
    "    return new_mat.astype(int)\n",
    "  \n",
    "\n",
    "def reduce_prune_counts(prune_count_matrix, frac_noise=1.0):\n",
    "    '''Reduce (multiply) all prune counts (non-diagonal) by frac_noise and\n",
    "    increase diagonal by the total amount reduced in each column to \n",
    "    preserve column counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    prune_count_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A counts of mislabeled examples in every class. For this function, it\n",
    "        does not matter what the rows or columns are, but the diagonal terms\n",
    "        reflect the number of correctly labeled examples.\n",
    "\n",
    "    frac_noise : float\n",
    "        When frac_of_noise = 1.0, return all \"confidently\" estimated noise indices.\n",
    "        Value in range (0, 1] that determines the fraction of noisy example \n",
    "        indices to return based on the following formula for example class k.\n",
    "        frac_of_noise * number_of_mislabeled_examples_in_class_k, or equivalently    \n",
    "        frac_of_noise * inverse_noise_rate_class_k * num_examples_with_s_equal_k'''\n",
    "\n",
    "    new_mat = prune_count_matrix * frac_noise\n",
    "    np.fill_diagonal(new_mat, prune_count_matrix.diagonal())\n",
    "    np.fill_diagonal(new_mat, prune_count_matrix.diagonal() + np.sum(prune_count_matrix - new_mat, axis=0))\n",
    "\n",
    "    # These are counts, so return a matrix of ints.\n",
    "    return new_mat.astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
