{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Estimation\n",
    "\n",
    "#### Contains methods for estimating four latent structures used for confident learning.\n",
    "* The latent prior of the unobserved, errorless labels $y$: denoted $p(y)$ (latex) & '```py```' (code).\n",
    "* The latent noisy channel (noise matrix) characterizing the flipping rates: denoted $P_{s \\vert y }$ (latex) & '```nm```' (code).\n",
    "* The latent inverse noise matrix characterizing flipping process: denoted $P_{y \\vert s}$ (latex) & '```inv```' (code).\n",
    "* The latent ```confident_joint```, an unnormalized counts matrix of counting a confident subset of the joint counts of label errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.linear_model import LogisticRegression as logreg\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "from confidentlearning.util import value_counts, clip_values, clip_noise_rates\n",
    "from confidentlearning.latent_algebra import compute_inv_noise_matrix, compute_py, compute_noise_matrix_from_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_confident_joint_from_probabilities(\n",
    "    s, \n",
    "    psx, \n",
    "    thresholds = None, \n",
    "    force_ps = False,\n",
    "    return_list_of_converging_cj_matrices = False,\n",
    "):\n",
    "    '''Estimates P(s,y), the confident counts of the latent \n",
    "    joint distribution of true and noisy labels \n",
    "    using observed s and predicted probabilities psx.\n",
    "\n",
    "    Important! This function assumes that psx are out-of-sample \n",
    "    holdout probabilities. This can be done with cross validation. If\n",
    "    the probabilities are not computed out-of-sample, overfitting may occur.\n",
    "\n",
    "    This function estimates the joint of shape (K, K). This is the\n",
    "    confident counts of examples in every class, labeled as every other class.\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, the estimate is within 1 percent of the truth.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "        A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "        the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    psx : np.array (shape (N, K))\n",
    "        P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "        This is the probability distribution over all K classes, for each\n",
    "        example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "        have been computed using 3 (or higher) fold cross-validation.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "        P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "        this threshold, it is counted as having hidden label y = k. This is \n",
    "        not used for pruning, only for estimating the noise rates using \n",
    "        confident counts. This value should be between 0 and 1. Default is None.\n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output confident_joint matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the confident_joint, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    return_list_of_converging_cj_matrices : bool (default = False)\n",
    "        When force_ps is true, it converges the joint count matrix that is returned.\n",
    "        Setting this to true will return the list of the converged matrices. The first\n",
    "        item in the list is the original and the last item is the final result.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "        confident_joint matrix count(s, y) : np.array (shape (K, K))'''\n",
    "    \n",
    "    # Number of classes\n",
    "    K = len(np.unique(s))  \n",
    "    # 'ps' is p(s=k)\n",
    "    ps = value_counts(s) / float(len(s))\n",
    "    # joint counts\n",
    "    cjs = []\n",
    "    \n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "    \n",
    "    sgd_epochs = force_ps if type(force_ps) == int else 1   \n",
    "    for sgd_iteration in range(sgd_epochs): \n",
    "        # Estimate the probability thresholds for confident counting \n",
    "        if thresholds is None:\n",
    "            thresholds = [np.mean(psx[:,k][s == k]) for k in range(K)] # P(s^=k|s=k)\n",
    "\n",
    "        # Confident examples are those that we are confident have label y = k\n",
    "        # Estimate the (K, K) matrix of confident examples having s = k_s and y = k_y\n",
    "        confident_joint = np.zeros((K,K))\n",
    "        for k_s in range(K): # k_s is the class value k of noisy label s\n",
    "            for k_y in range(K): # k_y is the (guessed) class value k of true label y\n",
    "                confident_joint[k_s][k_y] = sum((psx[:,k_y] >= thresholds[k_y]) & (s == k_s))\n",
    "        cjs.append(confident_joint)\n",
    "        \n",
    "        if force_ps:\n",
    "            joint_ps = confident_joint.sum(axis=1) / float(np.sum(confident_joint))\n",
    "            # Update thresholds (SGD) to converge p(s) of joint with actual p(s)    \n",
    "            eta = 0.5 # learning rate\n",
    "            thresholds += eta * (joint_ps - ps)\n",
    "        else: # Do not converge p(s) of joint with actual p(s)\n",
    "            break\n",
    "            \n",
    "    return cjs if return_list_of_converging_cj_matrices else confident_joint\n",
    " \n",
    "    \n",
    "def estimate_latent(\n",
    "    confident_joint, \n",
    "    s, \n",
    "    py_method = 'cnt', \n",
    "    converge_latent_estimates = False,\n",
    "):\n",
    "    '''Computes the latent prior p(y), the noise matrix P(s|y) and the\n",
    "    inverse noise matrix P(y|s) from the confident_joint count(s, y). The\n",
    "    confident_joint estimated by estimate_confident_joint_from_probabilities()\n",
    "    by counting confident examples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "        A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "        the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "        \n",
    "    confident_joint : np.array (shape (K, K), type int)\n",
    "        A K,K integer matrix of count(s=k, y=k). Estimatesa a confident subset of\n",
    "        the joint disribution of the noisy and true labels P_{s,y}.\n",
    "        Each entry in the matrix contains the number of examples confidently \n",
    "        counted into every pair (s=j, y=k) classes.\n",
    "        \n",
    "    py_method : str\n",
    "        How to compute the latent prior p(y=k). Default is \"cnt\" as it tends to\n",
    "        work best, but you may also set this hyperparameter to \"eqn\" or \"marginal\".\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent. '''\n",
    "    \n",
    "    # Number of classes\n",
    "    K = len(np.unique(s))  \n",
    "    # 'ps' is p(s=k)\n",
    "    ps = value_counts(s) / float(len(s))\n",
    "    \n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "    \n",
    "    # Number of training examples confidently counted from each noisy class\n",
    "    s_count = confident_joint.sum(axis=1).astype(float)\n",
    "    \n",
    "    # Number of training examples confidently counted into each true class\n",
    "    y_count = confident_joint.sum(axis=0).astype(float)\n",
    "    \n",
    "    # Confident Counts Estimator for p(s=k_s|y=k_y) ~ |s=k_s and y=k_y| / |y=k_y|\n",
    "    noise_matrix = confident_joint / y_count\n",
    "\n",
    "    # Confident Counts Estimator for p(y=k_y|s=k_s) ~ |y=k_y and s=k_s| / |s=k_s|\n",
    "    inv_noise_matrix = confident_joint.T / s_count\n",
    "    \n",
    "    if py_method == 'cnt': \n",
    "        py = (y_count / s_count) * ps\n",
    "        # Equivalently,\n",
    "        # py = inv_noise_matrix.diagonal() / noise_matrix.diagonal() * ps\n",
    "    elif py_method == 'eqn':\n",
    "        py = np.linalg.inv(noise_matrix).dot(ps)\n",
    "    elif py_method == 'marginal':\n",
    "        py = y_count / float(sum(y_count))\n",
    "    else:\n",
    "        raise ValueError('py_method parameter should be cnt, eqn, or marginal')\n",
    "    \n",
    "    # Clip py and noise rates into proper range [0,1)\n",
    "    # For py, no class should have probability 0 so we use 1e-5\n",
    "    py = clip_values(py, low=1e-5, high=1.0, new_sum = 1.0)\n",
    "    noise_matrix = clip_noise_rates(noise_matrix) \n",
    "    inv_noise_matrix = clip_noise_rates(inv_noise_matrix)\n",
    "\n",
    "    if converge_latent_estimates:\n",
    "        py, noise_matrix, inv_noise_matrix = converge_estimates(ps, py, noise_matrix, inv_noise_matrix)\n",
    "        # Again clip py and noise rates into proper range [0,1)\n",
    "        py = clip_values(py, low=1e-5, high=1.0, new_sum = 1.0) \n",
    "        noise_matrix = clip_noise_rates(noise_matrix) \n",
    "        inv_noise_matrix = clip_noise_rates(inv_noise_matrix)\n",
    "\n",
    "    return py, noise_matrix, inv_noise_matrix                  \n",
    "    \n",
    "    \n",
    "def estimate_py_and_noise_matrices_from_probabilities(\n",
    "    s, \n",
    "    psx, \n",
    "    thresholds = None,\n",
    "    converge_latent_estimates = True,\n",
    "    force_ps = False,\n",
    "    py_method = 'cnt', \n",
    "):\n",
    "    '''Computes the confident counts\n",
    "    estimate of latent variables py and the noise rates \n",
    "    using observed s and predicted probabilities psx.\n",
    "\n",
    "    Important! This function assumes that psx are out-of-sample \n",
    "    holdout probabilities. This can be done with cross validation. If\n",
    "    the probabilities are not computed out-of-sample, overfitting may occur.\n",
    "\n",
    "    This function estimates the noise_matrix of shape (K, K). This is the\n",
    "    fraction of examples in every class, labeled as every other class. The\n",
    "    noise_matrix is a conditional probability matrix for P(s=k_s|y=k_y).\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    psx : np.array (shape (N, K))\n",
    "      P(s=k|x) is a matrix with K (noisy) probabilities for each of the N examples x.\n",
    "      This is the probability distribution over all K classes, for each\n",
    "      example, regarding whether the example has label s==k P(s=k|x). psx should\n",
    "      have been computed using 3 (or higher) fold cross-validation.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent. \n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output confident_joint matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the confident_joint, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    py_method : str\n",
    "        How to compute the latent prior p(y=k). Default is \"cnt\" as it tends to\n",
    "        work best, but you may also set this hyperparameter to \"eqn\" or \"marginal\".\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "        py, noise_matrix, inverse_noise_matrix'''\n",
    "  \n",
    "    confident_joint = estimate_confident_joint_from_probabilities(s, psx, thresholds, force_ps)\n",
    "    py, noise_matrix, inv_noise_matrix = estimate_latent(        \n",
    "        confident_joint=confident_joint, \n",
    "        s=s, \n",
    "        py_method=py_method, \n",
    "        converge_latent_estimates=converge_latent_estimates,\n",
    "    )\n",
    "    \n",
    "    return py, noise_matrix, inv_noise_matrix, confident_joint\n",
    "\n",
    "\n",
    "def estimate_confident_joint_and_cv_pred_proba(\n",
    "    X, \n",
    "    s, \n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    thresholds = None,\n",
    "    force_ps = False,\n",
    "    return_list_of_converging_cj_matrices = False,\n",
    "    seed = None,\n",
    "):\n",
    "    '''Estimates P(s,y), the confident counts of the latent \n",
    "    joint distribution of true and noisy labels \n",
    "    using observed s and predicted probabilities psx. \n",
    "\n",
    "    The output of this function is a numpy array of shape (K, K). \n",
    "\n",
    "    Under certain conditions, estimates are exact, and in many\n",
    "    conditions, estimates are within one percent of actual.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output confident_joint matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the confident_joint, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    return_list_of_converging_cj_matrices : bool (default = False)\n",
    "        When force_ps is true, it converges the joint count matrix that is returned.\n",
    "        Setting this to true will return the list of the converged matrices. The first\n",
    "        item in the list is the original and the last item is the final result.\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "      Returns a tuple of two numpy array matrices in the form:\n",
    "      (joint counts matrix, predicted probability matrix)'''\n",
    "  \n",
    "    # Number of classes\n",
    "    K = len(np.unique(s))  \n",
    "    # 'ps' is p(s=k)\n",
    "    ps = value_counts(s) / float(len(s))\n",
    "    \n",
    "    # Ensure labels are of type np.array()\n",
    "    s = np.asarray(s)\n",
    "\n",
    "    # Create cross-validation object for out-of-sample predicted probabilities.\n",
    "    # CV folds preserve the fraction of noisy positive and\n",
    "    # noisy negative examples in each class.\n",
    "    kf = StratifiedKFold(n_splits = cv_n_folds, shuffle = True, random_state = seed)\n",
    "\n",
    "    # Intialize result storage and final psx array\n",
    "    confident_joint_per_cv_fold = []\n",
    "    psx = np.zeros((len(s), K))\n",
    "\n",
    "    # Split X and s into \"cv_n_folds\" stratified folds.\n",
    "    for k, (cv_train_idx, cv_holdout_idx) in enumerate(kf.split(X, s)):\n",
    "\n",
    "        # Select the training and holdout cross-validated sets.\n",
    "        X_train_cv, X_holdout_cv = X[cv_train_idx], X[cv_holdout_idx]\n",
    "        s_train_cv, s_holdout_cv = s[cv_train_idx], s[cv_holdout_idx]\n",
    "\n",
    "        # Fit the clf classifier to the training set and \n",
    "        # predict on the holdout set and update psx. \n",
    "        clf.fit(X_train_cv, s_train_cv)\n",
    "        psx_cv = clf.predict_proba(X_holdout_cv) # P(s = k|x) # [:,1]\n",
    "        psx[cv_holdout_idx] = psx_cv\n",
    "\n",
    "        # Compute and append the confident counts noise estimators\n",
    "        # to estimate the positive and negative mislabeling rates.\n",
    "        confident_joint_cv = estimate_confident_joint_from_probabilities(\n",
    "            s = s_holdout_cv, \n",
    "            psx = psx_cv, # P(s = k|x)\n",
    "            thresholds = thresholds, \n",
    "            force_ps = force_ps,\n",
    "            return_list_of_converging_cj_matrices = return_list_of_converging_cj_matrices,\n",
    "        )\n",
    "        \n",
    "        confident_joint_per_cv_fold.append(confident_joint_cv)\n",
    "    \n",
    "    if return_list_of_converging_cj_matrices:\n",
    "        jcs = [np.sum(np.stack(jcs), axis=0) for jcs in zip(*confident_joint_per_cv_fold)] \n",
    "        return jcs, psx\n",
    "\n",
    "    confident_joint = np.sum(np.stack(confident_joint_per_cv_fold), axis=0)\n",
    "    return confident_joint, psx\n",
    "\n",
    "\n",
    "def estimate_py_noise_matrices_and_cv_pred_proba(\n",
    "    X, \n",
    "    s, \n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    thresholds = None,\n",
    "    converge_latent_estimates = False,\n",
    "    force_ps = False,\n",
    "    return_list_of_converging_cj_matrices = False,\n",
    "    py_method = 'cnt',\n",
    "    seed = None,\n",
    "):\n",
    "    '''This function computes the out-of-sample predicted \n",
    "    probability P(s=k|x) for every example x in X using cross\n",
    "    validation while also computing the confident counts noise\n",
    "    rates within each cross-validated subset and returning\n",
    "    the average noise rate across all examples. \n",
    "\n",
    "    This function estimates the noise_matrix of shape (K, K). This is the\n",
    "    fraction of examples in every class, labeled as every other class. The\n",
    "    noise_matrix is a conditional probability matrix for P(s=k_s|y=k_y).\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling. \"s\" denotes\n",
    "      the noisy label instead of \\tilde(y), for ASCII encoding reasons.\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent.\n",
    "        \n",
    "    force_ps : bool or int\n",
    "        If true, forces the output confident_joint matrix to have p(s) closer to the true\n",
    "        p(s). The method used is SGD with a learning rate of eta = 0.5.\n",
    "        If force_ps is an integer, it represents the number of epochs.\n",
    "        Setting this to True is not always good. To make p(s) match, fewer confident\n",
    "        examples are used to estimate the confident_joint, resulting in poorer estimation of\n",
    "        the overall matrix even if p(s) is more accurate. \n",
    "        \n",
    "    return_list_of_converging_cj_matrices : bool (default = False)\n",
    "        When force_ps is true, it converges the joint count matrix that is returned.\n",
    "        Setting this to true will return the list of the converged matrices. The first\n",
    "        item in the list is the original and the last item is the final result.\n",
    "        \n",
    "    py_method : str\n",
    "        How to compute the latent prior p(y=k). Default is \"cnt\" as it tends to\n",
    "        work best, but you may also set this hyperparameter to \"eqn\" or \"marginal\".\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "      Returns a tuple of five numpy array matrices in the form:\n",
    "      (py, noise_matrix, inverse_noise_matrix, \n",
    "      joint count matrix i.e. confident joint, predicted probability matrix)'''\n",
    "    \n",
    "    confident_joint, psx = estimate_confident_joint_and_cv_pred_proba(\n",
    "        X = X, \n",
    "        s = s, \n",
    "        clf = clf,\n",
    "        cv_n_folds = cv_n_folds,\n",
    "        thresholds = thresholds,\n",
    "        force_ps = force_ps,\n",
    "        return_list_of_converging_cj_matrices = return_list_of_converging_cj_matrices,\n",
    "        seed = seed,\n",
    "    )\n",
    "    \n",
    "    py, noise_matrix, inv_noise_matrix = estimate_latent(\n",
    "        confident_joint = confident_joint, \n",
    "        s = s, \n",
    "        py_method = py_method, \n",
    "        converge_latent_estimates = converge_latent_estimates,\n",
    "    )\n",
    "    \n",
    "    return py, noise_matrix, inv_noise_matrix, confident_joint, psx \n",
    "\n",
    "\n",
    "def estimate_cv_predicted_probabilities(\n",
    "    X, \n",
    "    labels, # class labels can be noisy (s) or not noisy (y).\n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    seed = None,\n",
    "):\n",
    "    '''This function computes the out-of-sample predicted \n",
    "    probability [P(s=k|x)] for every example in X using cross\n",
    "    validation. Output is a np.array of shape (N, K) where N is \n",
    "    the number of training examples and K is the number of classes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    labels : np.array or list of ints from [0,1,..,K-1]\n",
    "      A discrete vector of class labels which may or may not contain mislabeling\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.\n",
    "    '''\n",
    "\n",
    "    return estimate_py_noise_matrices_and_cv_pred_proba(\n",
    "        X = X, \n",
    "        s = labels, \n",
    "        clf = clf,\n",
    "        cv_n_folds = cv_n_folds,\n",
    "        seed = seed,\n",
    "    )[-1]\n",
    "\n",
    "\n",
    "def estimate_noise_matrices(\n",
    "    X, \n",
    "    s, \n",
    "    clf = logreg(),\n",
    "    cv_n_folds = 5,\n",
    "    thresholds = None,\n",
    "    converge_latent_estimates = True,\n",
    "    seed = None,\n",
    "):\n",
    "    '''Estimates the noise_matrix of shape (K, K). This is the\n",
    "    fraction of examples in every class, labeled as every other class. The\n",
    "    noise_matrix is a conditional probability matrix for P(s=k_s|y=k_y).\n",
    "\n",
    "    Under certain conditions, estimates are exact, and in most\n",
    "    conditions, estimates are within one percent of the actual noise rates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.array\n",
    "      Input feature matrix (N, D), 2D numpy array\n",
    "\n",
    "    s : np.array\n",
    "      A discrete vector of labels, s, which may contain mislabeling\n",
    "\n",
    "    clf : sklearn.classifier or equivalent\n",
    "      Default classifier used is logistic regression. Assumes clf\n",
    "      has predict_proba() and fit() defined.\n",
    "\n",
    "    cv_n_folds : int\n",
    "      The number of cross-validation folds used to compute\n",
    "      out-of-sample probabilities for each example in X.\n",
    "\n",
    "    thresholds : iterable (list or np.array) of shape (K, 1)  or (K,)\n",
    "      P(s^=k|s=k). If an example has a predicted probability \"greater\" than \n",
    "      this threshold, it is counted as having hidden label y = k. This is \n",
    "      not used for pruning, only for estimating the noise rates using \n",
    "      confident counts. This value should be between 0 and 1. Default is None.\n",
    "\n",
    "    converge_latent_estimates : bool\n",
    "      If true, forces numerical consistency of estimates. Each is estimated\n",
    "      independently, but they are related mathematically with closed form \n",
    "      equivalences. This will iteratively make them mathematically consistent.\n",
    "        \n",
    "    seed : int (default = None)\n",
    "        Number to set the default state of the random number generator used to split \n",
    "        the cross-validated folds. If None, uses np.random current random state.'''\n",
    "\n",
    "    return estimate_py_noise_matrices_and_cv_pred_proba(\n",
    "        X = X, \n",
    "        s = s, \n",
    "        clf = clf,\n",
    "        cv_n_folds = cv_n_folds,\n",
    "        thresholds = thresholds,\n",
    "        converge_latent_estimates = converge_latent_estimates,\n",
    "        seed = seed,\n",
    "    )[1:-2]\n",
    "\n",
    "\n",
    "def converge_estimates(\n",
    "    ps,\n",
    "    py,\n",
    "    noise_matrix, \n",
    "    inverse_noise_matrix, \n",
    "    inv_noise_matrix_iterations = 5,\n",
    "    noise_matrix_iterations = 3,\n",
    "):\n",
    "    '''Computes py := P(y=k) and both noise_matrix and inverse_noise_matrix,\n",
    "    by numerically converging ps := P(s=k), py, and the noise matrices.\n",
    "\n",
    "    Forces numerical consistency of estimates. Each is estimated\n",
    "    independently, but they are related mathematically with closed form \n",
    "    equivalences. This will iteratively make them mathematically consistent. \n",
    "\n",
    "    py := P(y=k) and the inverse noise matrix P(y=k_y|s=k_s) specify one another, \n",
    "    meaning one can be computed from the other and vice versa. When numerical\n",
    "    discrepancy exists due to poor estimation, they can be made to agree by repeatedly\n",
    "    computing one from the other, for some a certain number of iterations (3-10 works fine.)\n",
    "\n",
    "    Do not set iterations too high or performance will decrease as small deviations\n",
    "    will get perturbated over and over and potentially magnified.\n",
    "\n",
    "    Note that we have to first converge the inverse_noise_matrix and py, \n",
    "    then we can update the noise_matrix, then repeat. This is becauase the\n",
    "    inverse noise matrix depends on py (which is unknown/latent), but the\n",
    "    noise matrix depends on ps (which is known), so there will be no change\n",
    "    in the noise matrix if we recompute it when py and inverse_noise_matrix change.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    ps : np.array (shape (K, ) or (1, K))\n",
    "        The fraction (prior probability) of each observed, noisy class label, P(y = k).\n",
    "\n",
    "    noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A conditional probablity matrix of the form P(s=k_s|y=k_y) containing\n",
    "        the fraction of examples in every class, labeled as every other class.\n",
    "        Assumes columns of noise_matrix sum to 1.\n",
    "\n",
    "    inverse_noise_matrix : np.array of shape (K, K), K = number of classes \n",
    "        A conditional probablity matrix of the form P(y=k_y|s=k_s) representing\n",
    "        the estimated fraction observed examples in each class k_s, that are\n",
    "        mislabeled examples from every other class k_y. If None, the \n",
    "        inverse_noise_matrix will be computed from psx and s.\n",
    "        Assumes columns of inverse_noise_matrix sum to 1.\n",
    "\n",
    "    Output\n",
    "    ------  \n",
    "        Three np.arrays of the form (py, noise_matrix, inverse_noise_matrix) with py \n",
    "        and inverse_noise_matrix and noise_matrix having numerical agreement.'''  \n",
    "  \n",
    "    for j in range(noise_matrix_iterations):\n",
    "        for i in range(inv_noise_matrix_iterations):\n",
    "            inverse_noise_matrix = compute_inv_noise_matrix(py, noise_matrix, ps)\n",
    "            py = compute_py(ps, noise_matrix, inverse_noise_matrix)\n",
    "        noise_matrix = compute_noise_matrix_from_inverse(ps, inverse_noise_matrix, py)\n",
    "    \n",
    "    return py, noise_matrix, inverse_noise_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
